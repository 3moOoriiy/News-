import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import time
from docx import Document
from fpdf import FPDF

# -------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© --------
st.set_page_config(page_title="ðŸ“° Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©", layout="wide")

# -------- ØªÙ„Ø®ÙŠØµ Ø³Ø±ÙŠØ¹ --------
def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

# -------- ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ --------
def analyze_sentiment(text):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    if polarity > 0.1:
        return "ðŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ðŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ðŸ˜ Ù…Ø­Ø§ÙŠØ¯"

# -------- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± --------
def fetch_news(rss_links, keywords, date_from, date_to):
    all_news = []
    for source, url in rss_links.items():
        feed = feedparser.parse(url)
        for entry in feed.entries:
            title = entry.title
            summary = entry.get("summary", "")
            link = entry.link
            published = entry.get("published", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            try:
                published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
            except:
                published_dt = datetime.now()
            image = ""
            if 'media_content' in entry:
                image = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry:
                image = entry.media_thumbnail[0].get('url', '')

            # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
            if not (date_from <= published_dt.date() <= date_to):
                continue

            # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            if keywords:
                if not any(keyword.lower() in (title + summary).lower() for keyword in keywords):
                    continue

            all_news.append({
                "source": source,
                "title": title,
                "summary": summary,
                "link": link,
                "published": published_dt,
                "image": image,
                "sentiment": analyze_sentiment(summary)
            })
    return all_news

# -------- ØªØµØ¯ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± --------
def export_to_word(news_list):
    doc = Document()
    for news in news_list:
        doc.add_heading(news['title'], level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}")
        doc.add_paragraph(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
        doc.add_paragraph(f"ðŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
        doc.add_paragraph("-----")
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer

def export_to_html(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    buffer.write(df.to_html().encode())
    buffer.seek(0)
    return buffer

# -------- ÙˆØ§Ø¬Ù‡Ø© Streamlit --------
st.title("ðŸ—žï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø°ÙƒÙŠØ©")

# -------- Ø§Ù„Ù…ØµØ§Ø¯Ø± --------
rss_sources = {
    "BBC Ø¹Ø±Ø¨ÙŠ": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "CNN Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "http://arabic.cnn.com/rss/latest",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": "https://aawsat.com/home/rss.xml"
}

col1, col2 = st.columns([1, 2])

with col1:
    selected_sources = st.multiselect("ðŸŒ Ø§Ø®ØªØ± Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(rss_sources.keys()), default=list(rss_sources.keys()))
    keywords_input = st.text_input("ðŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
    keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []
    date_from = st.date_input("ðŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
    date_to = st.date_input("ðŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())
    auto_refresh = st.checkbox("â™»ï¸ ØªØ­Ø¯ÙŠØ« ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ 60 Ø«Ø§Ù†ÙŠØ©")
    run = st.button("ðŸ“¥ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

if auto_refresh:
    time.sleep(60)
    st.experimental_rerun()

with col2:
    if run:
        selected_rss_links = {src: rss_sources[src] for src in selected_sources}
        news = fetch_news(selected_rss_links, keywords, date_from, date_to)

        if not news:
            st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ·.")
        else:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø±.")
            # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Grid
            show_count = st.session_state.get("show_count", 6)
            news_to_display = news[:show_count]

            for i in range(0, len(news_to_display), 3):
                cols = st.columns(3)
                for idx, col in enumerate(cols):
                    if i + idx < len(news_to_display):
                        item = news_to_display[i + idx]
                        with col:
                            if item['image']:
                                st.image(item['image'], width=200)
                            st.markdown(f"### {item['title']}")
                            st.markdown(f"**ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®:** {item['published'].strftime('%Y-%m-%d')}")
                            st.markdown(f"**Ø§Ù„Ù…ØµØ¯Ø±:** {item['source']}")
                            st.markdown(f"**ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ:** {summarize(item['summary'])}")
                            st.markdown(f"**ØªØ­Ù„ÙŠÙ„:** {item['sentiment']}")
                            st.markdown(f"[Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯ â†—]({item['link']})")

            if show_count < len(news):
                if st.button("Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯"):
                    st.session_state["show_count"] = show_count + 6

            # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
            st.download_button("ðŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word", data=export_to_word(news), file_name="news.docx")
            st.download_button("ðŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel", data=export_to_excel(news), file_name="news.xlsx")
            st.download_button("ðŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ HTML", data=export_to_html(news), file_name="news.html")

            # Ø±Ø³Ù… Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            all_text = " ".join(item['summary'] for item in news)
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)
            st.header("â˜ï¸ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§")
            st.image(wordcloud.to_array())

            # Ø±Ø³Ù… Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            df = pd.DataFrame(news)
            if not df.empty:
                st.header("ðŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±")
                st.bar_chart(df['source'].value_counts())

                st.header("ðŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù†ÙˆÙŠ")
                st.bar_chart(df['sentiment'].value_counts())
