import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
import urllib.request
import urllib.parse
import json
import re
import time

st.set_page_config(page_title="๐ฐ ุฃุฏุงุฉ ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุงูุฐููุฉ", layout="wide")
st.title("๐๏ธ ุฃุฏุงุฉ ุฅุฏุงุฑุฉ ูุชุญููู ุงูุฃุฎุจุงุฑ ุงููุชุทูุฑุฉ (RSS + Web Scraping)")

# ุงูุชุตูููุงุช ุงููุญุณููุฉ ูุงูููุณุนุฉ (ููุชุตููู ุงูุชููุงุฆู ููุท)
category_keywords = {
    "ุณูุงุณุฉ": ["ุฑุฆูุณ", "ูุฒูุฑ", "ุงูุชุฎุงุจุงุช", "ุจุฑููุงู", "ุณูุงุณุฉ", "ุญูููุฉ", "ูุงุฆุจ", "ูุฌูุณ", "ุฏููุฉ", "ุญุฒุจ", "ุณููุฑ", "ูุฒุงุฑุฉ", "ูุงููู", "ุฏุณุชูุฑ", "ูุญููุฉ", "ูุงุถู", "ุนุฏุงูุฉ", "ุฃูู", "ุดุฑุทุฉ", "ุฌูุด", "ุนุณูุฑู", "ุฏูุงุน", "ููุงุช", "ุงุณุชุฑุงุชูุฌูุฉ", "ุฏุจูููุงุณูุฉ", "ูุนุงูุฏุฉ", "ุงุชูุงููุฉ", "ูุคุชูุฑ", "ููุฉ", "ุญูุงุฑ", "ููุงูุถุงุช", "ุชุญุงูู", "ูุนุงุฑุถุฉ", "ุซูุฑุฉ", "ุงูููุงุจ", "ุฏูููุฑุงุทูุฉ", "ุญููู", "ุญุฑูุงุช", "ููุงุทู", "ุดุนุจ", "ุฃูุฉ", "ูุทู", "ูููู", "ุนุฑุจู", "ุฅุณูุงูู", "ุฏููู", "ุฅููููู", "ูุญูู", "ุจูุฏูุฉ", "ูุญุงูุธุฉ", "ููุงูุฉ", "ูุฏููุฉ", "ุนุงุตูุฉ"],
    
    "ุฑูุงุถุฉ": ["ูุฑุฉ", "ูุงุนุจ", "ูุจุงุฑุงุฉ", "ุฏูุฑู", "ูุฏู", "ูุฑูู", "ุจุทููุฉ", "ุฑูุงุถุฉ", "ููุนุจ", "ุชุฏุฑูุจ", "ูุฏุฑุจ", "ุญูู", "ูุงุฏู", "ุจุทู", "ูุฃุณ", "ุฌุงุฆุฒุฉ", "ููุฏุงููุฉ", "ุฐูุจูุฉ", "ูุถูุฉ", "ุจุฑููุฒูุฉ", "ุฃูููุจูุงุฏ", "ูุฑุฉ ูุฏู", "ูุฑุฉ ุณูุฉ", "ูุฑุฉ ุทุงุฆุฑุฉ", "ุชูุณ", "ุณุจุงุญุฉ", "ุฌุฑู", "ูุตุงุฑุนุฉ", "ููุงููุฉ", "ุฌูุจุงุฒ", "ุฃูุนุงุจ ููู", "ุณุจุงู", "ูุงุฑุงุซูู", "ููุฌุง", "ููุงูุฉ", "ุตุญุฉ ุจุฏููุฉ", "ุชุบุฐูุฉ ุฑูุงุถูุฉ", "ููููุงุช", "ุจุทููุฉ ุงูุนุงูู", "ูุฃุณ ุงูุนุงูู", "ุฏูุฑู ุฃุจุทุงู", "ุงูุฏูุฑู ุงูููุชุงุฒ", "ุงูุฏูุฑู ุงููุญูู", "ููุชุฎุจ", "ูุตุฑ", "ุงูุณุนูุฏูุฉ", "ุงูุฅูุงุฑุงุช", "ูุทุฑ", "ุงูุนุฑุงู", "ุงูุฃุฑุฏู", "ูุจูุงู", "ุงููุบุฑุจ", "ุชููุณ", "ุงูุฌุฒุงุฆุฑ"],
    
    "ุงูุชุตุงุฏ": ["ุณูู", "ุงูุชุตุงุฏ", "ุงุณุชุซูุงุฑ", "ุจูู", "ูุงู", "ุชุฌุงุฑุฉ", "ุตูุงุนุฉ", "ููุท", "ุบุงุฒ", "ุจูุฑุตุฉ", "ุฃุณูู", "ุนููุฉ", "ุฏููุงุฑ", "ููุฑู", "ุฑูุงู", "ุฏุฑูู", "ุฏููุงุฑ", "ุฌููู", "ููุฑุฉ", "ุชุถุฎู", "ุฑููุฏ", "ููู", "ุฅูุชุงุฌ", "ุชุตุฏูุฑ", "ุงุณุชูุฑุงุฏ", "ููุฒุงููุฉ", "ุนุฌุฒ", "ูุงุฆุถ", "ุฏููู", "ูุฑูุถ", "ููุงุฆุฏ", "ูุตุงุฑู", "ุจููู", "ุชูููู", "ุงุฆุชูุงู", "ุฐูุจ", "ูุถุฉ", "ูุนุงุฏู", "ุทุงูุฉ", "ููุฑุจุงุก", "ููุงู", "ุฒุฑุงุนุฉ", "ุตูุฏ", "ุณูุงุญุฉ", "ููุงุฏู", "ุทูุฑุงู", "ุดุญู", "ููุงุตูุงุช", "ุงุชุตุงูุงุช", "ุชูููุฉ ูุงููุฉ", "ุนููุฉ ุฑูููุฉ", "ุจูุชูููู", "ุจููู ุชุดูู", "ุฐูุงุก ุงุตุทูุงุนู", "ุฑูุจูุช", "ุฃุชูุชุฉ", "ุตูุงุนุฉ 4.0", "ุชุญูู ุฑููู", "ุฑูุงุฏุฉ ุฃุนูุงู", "ุดุฑูุงุช ูุงุดุฆุฉ", "ุงุญุชูุงุฑ", "ููุงูุณุฉ", "ุฃุณุนุงุฑ", "ุชูููุฉ", "ุฑุจุญ", "ุฎุณุงุฑุฉ", "ูุจูุนุงุช", "ุฅูุฑุงุฏุงุช", "ูุตุฑููุงุช"],
    
    "ุชูููููุฌูุง": ["ุชูููุฉ", "ุชุทุจูู", "ูุงุชู", "ุฐูุงุก", "ุจุฑูุฌุฉ", "ุฅูุชุฑูุช", "ุฑููู", "ุญุงุณูุจ", "ุดุจูุฉ", "ุขูููู", "ุฃูุฏุฑููุฏ", "ุณุงูุณููุบ", "ููุงูู", "ุฃุจู", "ุฌูุฌู", "ูุงููุฑูุณููุช", "ููุณุจูู", "ุชููุชุฑ", "ุฅูุณุชุบุฑุงู", "ููุชููุจ", "ุชูู ุชูู", "ูุงุชุณุงุจ", "ุชูุบุฑุงู", "ุณูุงุจ ุดุงุช", "ููููุฏ ุฅู", "ุจุฑูุงูุฌ", "ุชุทุจูู", "ูููุน", "ููุตุฉ", "ุฎูุงุฑุฒููุฉ", "ุจูุงูุงุช", "ุชุญููู", "ุฅุญุตุงุก", "ูุงุนุฏุฉ ุจูุงูุงุช", "ุฎุงุฏู", "ุณุญุงุจุฉ", "ุชุฎุฒูู", "ุฃูุงู", "ุญูุงูุฉ", "ููุฑูุณ", "ูุงูุฑ", "ุงุฎุชุฑุงู", "ุชุดููุฑ", "ูููุฉ ูุฑูุฑ", "ูููุฉ ุฑูููุฉ", "ุจุตูุฉ", "ูุฌู", "ุตูุช", "ูุงูุน ุงูุชุฑุงุถู", "ูุงูุน ูุนุฒุฒ", "ุทุจุงุนุฉ ุซูุงุซูุฉ ุงูุฃุจุนุงุฏ", "ุฑูุจูุช", "ุฐูุงุก ุงุตุทูุงุนู", "ุชุนูู ุขูุฉ", "ุดุจูุฉ ุนุตุจูุฉ", "ูุนุงูุฌุฉ ูุบุฉ", "ุฑุคูุฉ ุญุงุณูุจูุฉ", "ุฅูุชุฑูุช ุงูุฃุดูุงุก", "ุงูุจููู ุชุดูู", "ุนููุฉ ุฑูููุฉ", "NFT", "ููุชุงููุฑุณ", "ุฃูุนุงุจ", "ุจุซ", "ูุญุชูู ุฑููู", "ูุณุงุฆุท ูุชุนุฏุฏุฉ", "ููุฏูู", "ุตูุช", "ุตูุฑุฉ", "ุฌุฑุงููู", "ุชุตููู", "ูููุชุงุฌ", "ุชุญุฑูุฑ"],
    
    "ุตุญุฉ": ["ุทุจ", "ูุฑุถ", "ุนูุงุฌ", "ูุณุชุดูู", "ุฏูุงุก", "ุตุญุฉ", "ุทุจูุจ", "ููุฑูุณ", "ููุงุญ", "ูุจุงุก", "ููุฑุถ", "ุทุจูุจ ุฃุณูุงู", "ุตูุฏูู", "ูุฎุชุจุฑ", "ุชุญููู", "ูุญุต", "ุฃุดุนุฉ", "ุฌุฑุงุญุฉ", "ุนูููุฉ", "ุชุฎุฏูุฑ", "ูุฑูุถ", "ุฅุณุนุงู", "ุทูุงุฑุฆ", "ุนูุงูุฉ ูุฑูุฒุฉ", "ููุจ", "ุฑุฆุฉ", "ูุจุฏ", "ููู", "ุฏูุงุบ", "ุนุธุงู", "ุนุถูุงุช", "ุฃุนุตุงุจ", "ุฌูุฏ", "ุนููู", "ุฃุฐู", "ุฃูู", "ุญูุฌุฑุฉ", "ุฃุณูุงู", "ูู", "ูุนุฏุฉ", "ุฃูุนุงุก", "ุจููุฑูุงุณ", "ุบุฏุฏ", "ูุฑูููุงุช", "ุฏู", "ุถุบุท", "ุณูุฑู", "ููููุณุชุฑูู", "ุณุฑุทุงู", "ูุฑู", "ุงูุชูุงุจ", "ุนุฏูู", "ุจูุชูุฑูุง", "ุทููููุงุช", "ุญุณุงุณูุฉ", "ููุงุนุฉ", "ูุถุงุฏุงุช ุญูููุฉ", "ูุณูู", "ูุถุงุฏ ุงูุชูุงุจ", "ููุชุงููู", "ูุนุฏู", "ุชุบุฐูุฉ", "ุญููุฉ", "ุฑุฌูู", "ุณููุฉ", "ูุญุงูุฉ", "ููุงูุฉ", "ุฑูุงุถุฉ", "ููุฌุง", "ุชุฃูู", "ุงุณุชุฑุฎุงุก", "ุตุญุฉ ููุณูุฉ", "ุงูุชุฆุงุจ", "ููู", "ุชูุชุฑ", "ููู", "ุฃุฑู", "ุญูู", "ููุงุฏุฉ", "ุฃุทูุงู", "ูุฑุงููุฉ", "ุดูุฎูุฎุฉ", "ููุงูุฉ", "ุชุทุนูู", "ูุธุงูุฉ", "ุชุนููู", "ููุงูุฉ"],
    
    "ุชุนููู": ["ุชุนููู", "ุฌุงูุนุฉ", "ูุฏุฑุณุฉ", "ุทุงูุจ", "ุฏุฑุงุณุฉ", "ูููุฉ", "ูุนูุฏ", "ุชุฑุจูุฉ", "ุฃูุงุฏููู", "ุจุญุซ", "ุฃุณุชุงุฐ", "ูุนูู", "ูุฏุฑุณ", "ูุญุงุถุฑ", "ุจุงุญุซ", "ุฏูุชูุฑ", "ูุงุฌุณุชูุฑ", "ุจูุงููุฑููุณ", "ุฏุจููู", "ุดูุงุฏุฉ", "ุฏุฑุฌุฉ ุนูููุฉ", "ุชุฎุตุต", "ูุณู", "ูุฑุน", "ููุงูุฌ", "ูุชุงุจ", "ูุฐูุฑุฉ", "ูุงุฌุจ", "ุงูุชุญุงู", "ุงุฎุชุจุงุฑ", "ุชูููู", "ุฏุฑุฌุงุช", "ูุชุงุฆุฌ", "ูุฌุงุญ", "ุฑุณูุจ", "ุชููู", "ููุงูุฃุฉ", "ููุญุฉ", "ุจุนุซุฉ", "ุฏูุฑุฉ", "ูุฑุดุฉ", "ูุคุชูุฑ", "ูุฏูุฉ", "ูุญุงุถุฑุฉ", "ุนุฑุถ", "ูุดุฑูุน", "ุฑุณุงูุฉ", "ุฃุทุฑูุญุฉ", "ุชุฌุฑุจุฉ", "ูุฎุชุจุฑ", "ููุชุจุฉ", "ูุชุจ", "ูุฑุงุฌุน", "ูุตุงุฏุฑ", "ุฅูุชุฑูุช", "ุชุนูู ุฅููุชุฑููู", "ุชุนููู ุนู ุจุนุฏ", "ููุตุฉ ุชุนููููุฉ", "ููุฏูู ุชุนูููู", "ูุญุชูู ุฑููู", "ูุบุฉ ุนุฑุจูุฉ", "ุฑูุงุถูุงุช", "ุนููู", "ููุฒูุงุก", "ููููุงุก", "ุฃุญูุงุก", "ุฌุบุฑุงููุง", "ุชุงุฑูุฎ", "ุฏูู", "ููุณูุฉ", "ุนูู ููุณ", "ุงุฌุชูุงุน", "ุงูุชุตุงุฏ", "ุณูุงุณุฉ", "ูุงููู", "ุทุจ", "ููุฏุณุฉ", "ุญุงุณูุจ", "ุจุฑูุฌุฉ", "ูููู", "ุฃุฏุจ", "ุดุนุฑ", "ูุตุฉ", "ุฑูุงูุฉ", "ูุณุฑุญ", "ุณูููุง", "ููุณููู", "ุฑุณู", "ูุญุช", "ุฎุท ุนุฑุจู", "ุฎุท", "ุชุตููู", "ุฅุนูุงู", "ุตุญุงูุฉ", "ุฅุฐุงุนุฉ", "ุชููุฒููู", "ุฅุนูุงู", "ุชุณููู", "ุชุฑุฌูุฉ", "ูุบุงุช ุฃุฌูุจูุฉ", "ุฅูุฌููุฒู", "ูุฑูุณู", "ุฃููุงูู", "ุฅุณุจุงูู", "ุตููู", "ูุงุจุงูู", "ุฑูุณู", "ุชุฑูู", "ูุงุฑุณู", "ุนุจุฑู", "ูุงุชููู", "ูููุงูู"]
}

# ุฏุงูุฉ ุงูุจุญุซ ุงูููุชูุญ ูู ุงููุตูุต - ูุญุฏุซุฉ ูุชููู ุฃูุซุฑ ูุฑููุฉ
def flexible_keyword_search(text, keywords):
    """
    ุจุญุซ ูุฑู ูุฐูู ูู ุงููุตูุต - ูุจุญุซ ุนู ุฃู ูููุฉ ููุชุงุญูุฉ ูู ุฃู ููุงู
    """
    if not text or not keywords:
        return True  # ุฅุฐุง ูู ุชูู ููุงู ูููุงุช ููุชุงุญูุฉุ ุงุนุฑุถ ูู ุงูุฃุฎุจุงุฑ
    
    text_lower = text.lower()
    
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ ูุชุญููู ุงููุต ููุจุญุซ
    text_cleaned = re.sub(r'\s+', ' ', text_lower).strip()
    
    # ุงูุจุญุซ ุงููุจุงุดุฑ ูู ุงููุต
    for keyword in keywords:
        keyword = keyword.strip().lower()
        if not keyword:
            continue
            
        # ุงูุจุญุซ ุงููุจุงุดุฑ
        if keyword in text_cleaned:
            return True
        
        # ุงูุจุญุซ ุจุงููููุงุช ุงููููุตูุฉ (ุฅุฐุง ูุงูุช ุงููููุฉ ุงูููุชุงุญูุฉ ุชุญุชูู ุนูู ูุณุงูุงุช)
        keyword_words = keyword.split()
        if len(keyword_words) > 1:
            # ุงูุจุญุซ ุนู ูู ูููุฉ ูููุตูุฉ
            all_words_found = all(word in text_cleaned for word in keyword_words)
            if all_words_found:
                return True
        
        # ุงูุจุญุซ ุจุงูุฌุฐูุฑ (ุชุฌุฑูุจู - ูููููุงุช ุงูุนุฑุจูุฉ)
        # ูุซุงู: "ูุชุจ" ูุฌุฏ "ูุชุงุจ", "ูุงุชุจ", "ููุชูุจ"
        if len(keyword) >= 3:
            root = keyword[:3]  # ุฃุฎุฐ ุฃูู 3 ุฃุญุฑู ูุฌุฐุฑ ุชูุฑูุจู
            if root in text_cleaned:
                return True
    
    return False

# ุฏุงูุฉ ุชุญุณูู ุงูุจุญุซ ุจุงููุฑุงุฏูุงุช ูุงููููุงุช ุฐุงุช ุงูุตูุฉ
def enhanced_keyword_search(text, keywords):
    """
    ุจุญุซ ูุญุณู ูุชุถูู ุงููุฑุงุฏูุงุช ูุงููููุงุช ุฐุงุช ุงูุตูุฉ
    """
    if not text or not keywords:
        return True
    
    # ูุงููุณ ุงููุฑุงุฏูุงุช ุงูุจุณูุท (ูููู ุชูุณูุนู)
    synonyms_dict = {
        "ุญุฑุจ": ["ูุชุงู", "ูุนุฑูุฉ", "ุตุฑุงุน", "ูุฒุงุน", "ุญุฑุจ"],
        "ุงูุชุตุงุฏ": ["ูุงู", "ุชุฌุงุฑุฉ", "ุงุณุชุซูุงุฑ", "ุจูุฑุตุฉ", "ุณูู"],
        "ุณูุงุณุฉ": ["ุญูููุฉ", "ุฏููุฉ", "ุฑุฆูุณ", "ูุฒูุฑ", "ุงูุชุฎุงุจุงุช"],
        "ุฑูุงุถุฉ": ["ูุฑุฉ", "ูุงุนุจ", "ูุจุงุฑุงุฉ", "ูุฑูู", "ุจุทููุฉ"],
        "ุชุนููู": ["ูุฏุฑุณุฉ", "ุฌุงูุนุฉ", "ุทุงูุจ", "ูุนูู", "ุฏุฑุงุณุฉ"],
        "ุตุญุฉ": ["ุทุจ", "ูุฑุถ", "ุนูุงุฌ", "ูุณุชุดูู", "ุฏูุงุก"],
        "ุชูููููุฌูุง": ["ุชูููุฉ", "ููุจููุชุฑ", "ุฅูุชุฑูุช", "ุจุฑูุฌุฉ", "ุฐูุงุก ุงุตุทูุงุนู"]
    }
    
    text_lower = text.lower()
    
    for keyword in keywords:
        keyword = keyword.strip().lower()
        if not keyword:
            continue
        
        # ุงูุจุญุซ ุงููุจุงุดุฑ
        if keyword in text_lower:
            return True
        
        # ุงูุจุญุซ ูู ุงููุฑุงุฏูุงุช
        for main_word, synonyms in synonyms_dict.items():
            if keyword == main_word or keyword in synonyms:
                for synonym in synonyms:
                    if synonym in text_lower:
                        return True
        
        # ุงูุจุญุซ ุงูุฌุฒุฆู ูููููุงุช ุงูุทูููุฉ
        if len(keyword) > 4:
            # ุชูุณูู ุงููููุฉ ุงูููุชุงุญูุฉ ุฅูู ุฃุฌุฒุงุก
            parts = [keyword[i:i+4] for i in range(len(keyword)-3)]
            for part in parts:
                if part in text_lower:
                    return True
    
    return False

# ุงูุฏูุงู ุงููุญุณููุฉ
def create_smart_summary(title, content, max_sentences=3):
    """ุฅูุดุงุก ููุฎุต ุฐูู ุฃุทูู"""
    if not content or content == title:
        # ุฅุฐุง ูู ููู ููุงู ูุญุชููุ ููุณุน ุงูุนููุงู
        words = title.split()
        if len(words) > 10:
            return " ".join(words[:15]) + "..."
        else:
            return title + " - ุชูุงุตูู ุฅุถุงููุฉ ูุชุงุญุฉ ูู ุงูููุงู ุงููุงูู."
    
    # ุชูุธูู ุงููุต
    content = re.sub(r'<[^>]+>', '', content)  # ุฅุฒุงูุฉ HTML tags
    content = re.sub(r'\s+', ' ', content).strip()  # ุชูุธูู ุงููุณุงูุงุช
    
    # ุชูุณูู ุฅูู ุฌูู
    sentences = re.split(r'[.!?]+', content)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    if not sentences:
        return title
    
    # ุฃุฎุฐ ุฃูู ุนุฏุฉ ุฌูู
    selected_sentences = sentences[:max_sentences]
    summary = ". ".join(selected_sentences)
    
    # ุงูุชุฃูุฏ ูู ุทูู ููุงุณุจ
    if len(summary.split()) < 20:
        # ุฅุถุงูุฉ ุงููุฒูุฏ ูู ุงูุฌูู ุฅุฐุง ูุงู ูุตูุฑ
        extra_sentences = sentences[max_sentences:max_sentences+2]
        if extra_sentences:
            summary += ". " + ". ".join(extra_sentences)
    
    # ูุทุน ุฅุฐุง ูุงู ุทููู ุฌุฏุงู
    words = summary.split()
    if len(words) > 80:
        summary = " ".join(words[:80]) + "..."
    
    return summary if summary else title

def analyze_sentiment(text):
    if not text:
        return "๐ ูุญุงูุฏ"
    try:
        polarity = TextBlob(text).sentiment.polarity
        if polarity > 0.1:
            return "๐ ุฅูุฌุงุจู"
        elif polarity < -0.1:
            return "๐ ุณูุจู"
        else:
            return "๐ ูุญุงูุฏ"
    except:
        return "๐ ูุญุงูุฏ"

def detect_category(text):
    if not text:
        return "ุบูุฑ ูุตููู"
    text_lower = text.lower()
    category_scores = {}
    
    for category, words in category_keywords.items():
        score = sum(1 for word in words if word in text_lower)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return "ุบูุฑ ูุตููู"

def safe_request(url, timeout=10):
    """ุทูุจ ุขูู ูุน ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        response = urllib.request.urlopen(req, timeout=timeout)
        return response.read().decode('utf-8', errors='ignore')
    except Exception as e:
        st.warning(f"ุฎุทุฃ ูู ุงููุตูู ูู {url}: {str(e)}")
        return None

def extract_news_from_html(html_content, source_name, base_url):
    """ุงุณุชุฎุฑุงุฌ ุงูุฃุฎุจุงุฑ ูู HTML ุจุทุฑููุฉ ุฐููุฉ"""
    if not html_content:
        return []
    
    news_list = []
    
    # ุงูุจุญุซ ุนู ุงูุนูุงููู ุงููุญุชููุฉ
    title_patterns = [
        r'<h[1-4][^>]*>(.*?)</h[1-4]>',
        r'<title[^>]*>(.*?)</title>',
        r'<a[^>]*title="([^"]+)"',
        r'<div[^>]*class="[^"]*title[^"]*"[^>]*>(.*?)</div>'
    ]
    
    # ุงูุจุญุซ ุนู ุงูุฑูุงุจุท
    link_patterns = [
        r'<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>',
        r'href="([^"]+)"'
    ]
    
    # ุงูุจุญุซ ุนู ุงููุตูุต ุงูุทูููุฉ (ููููุฎุตุงุช)
    content_patterns = [
        r'<p[^>]*>(.*?)</p>',
        r'<div[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</div>',
        r'<div[^>]*class="[^"]*summary[^"]*"[^>]*>(.*?)</div>'
    ]
    
    titles = []
    links = []
    contents = []
    
    for pattern in title_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        for match in matches:
            if isinstance(match, tuple):
                title = match[0] if match[0] else match[1] if len(match) > 1 else ""
            else:
                title = match
            title = re.sub(r'<[^>]+>', '', title).strip()
            if title and len(title) > 10 and len(title) < 200:
                titles.append(title)
    
    for pattern in link_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        for match in matches:
            if isinstance(match, tuple):
                link = match[0]
            else:
                link = match
            if link and not link.startswith('#') and not link.startswith('javascript:'):
                if link.startswith('/'):
                    link = base_url + link
                elif not link.startswith('http'):
                    link = base_url + '/' + link
                links.append(link)
    
    # ุงุณุชุฎุฑุงุฌ ุงููุตูุต ููููุฎุตุงุช
    for pattern in content_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        for match in matches:
            content = re.sub(r'<[^>]+>', '', str(match)).strip()
            if content and len(content) > 50:
                contents.append(content)
    
    # ุฏูุฌ ุงูุนูุงููู ูุงูุฑูุงุจุท ูุงููุญุชููุงุช
    for i, title in enumerate(titles[:10]):  # ุฃูู 10 ุฃุฎุจุงุฑ
        link = links[i] if i < len(links) else base_url
        content = contents[i] if i < len(contents) else title
        
        # ุฅูุดุงุก ููุฎุต ูุญุณู
        smart_summary = create_smart_summary(title, content)
        
        news_list.append({
            "source": source_name,
            "title": title,
            "summary": smart_summary,
            "link": link,
            "published": datetime.now(),
            "image": "",
            "sentiment": analyze_sentiment(smart_summary),
            "category": detect_category(title + " " + smart_summary),
            "extraction_method": "HTML Parsing"
        })
    
    return news_list

def fetch_rss_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """ุฌูุจ ุงูุฃุฎุจุงุฑ ูู RSS ูุน ุจุญุซ ููุชูุญ ูููููุงุช ุงูููุชุงุญูุฉ"""
    try:
        feed = feedparser.parse(url)
        news_list = []
        
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        for entry in feed.entries:
            try:
                title = entry.get('title', 'ุจุฏูู ุนููุงู')
                summary = entry.get('summary', entry.get('description', ''))
                content = entry.get('content', [{}])
                if content and isinstance(content, list) and len(content) > 0:
                    full_content = content[0].get('value', summary)
                else:
                    full_content = summary
                
                link = entry.get('link', '')
                published = entry.get('published', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                
                # ูุนุงูุฌุฉ ุงูุชุงุฑูุฎ
                try:
                    published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
                except:
                    try:
                        published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                    except:
                        published_dt = datetime.now()
                
                # ููุชุฑุฉ ุงูุชุงุฑูุฎ
                if not (date_from <= published_dt.date() <= date_to):
                    continue

                # ุฅูุดุงุก ููุฎุต ุฐูู ูุญุณู
                enhanced_summary = create_smart_summary(title, full_content)
                
                # ุงูุจุญุซ ุงูููุชูุญ ูู ุงููููุงุช ุงูููุชุงุญูุฉ
                full_text = title + " " + enhanced_summary
                if keywords and not enhanced_keyword_search(full_text, keywords):
                    continue

                # ููุชุฑุฉ ุงูุชุตููู
                auto_category = detect_category(full_text)
                if chosen_category != "ุงููู" and auto_category != chosen_category:
                    continue

                # ุงูุจุญุซ ุนู ุตูุฑุฉ
                image = ""
                if hasattr(entry, 'media_content') and entry.media_content:
                    image = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image = entry.media_thumbnail[0].get('url', '')

                news_list.append({
                    "source": source_name,
                    "title": title,
                    "summary": enhanced_summary,
                    "link": link,
                    "published": published_dt,
                    "image": image,
                    "sentiment": analyze_sentiment(enhanced_summary),
                    "category": auto_category,
                    "extraction_method": "RSS"
                })
                
            except Exception as e:
                continue
                
        return news_list
        
    except Exception as e:
        return []

def fetch_website_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """ุฌูุจ ุงูุฃุฎุจุงุฑ ูู ุงููููุน ูุจุงุดุฑุฉ ูุน ุจุญุซ ููุชูุญ"""
    try:
        st.info(f"๐ ุฌุงุฑู ุชุญููู ูููุน {source_name}...")
        
        # ุฌูุจ ูุญุชูู ุงูุตูุญุฉ
        html_content = safe_request(url)
        if not html_content:
            return []
        
        # ุงุณุชุฎุฑุงุฌ ุงูุฃุฎุจุงุฑ ูู HTML
        base_url = url.rstrip('/')
        news_list = extract_news_from_html(html_content, source_name, base_url)
        
        # ููุชุฑุฉ ุงููุชุงุฆุฌ ุจุงูุจุญุซ ุงูููุชูุญ
        filtered_news = []
        for news in news_list:
            # ุงูุจุญุซ ุงูููุชูุญ ูู ุงููููุงุช ุงูููุชุงุญูุฉ
            full_text = news['title'] + " " + news['summary']
            if keywords and not enhanced_keyword_search(full_text, keywords):
                continue
            
            # ููุชุฑุฉ ุงูุชุตููู
            if chosen_category != "ุงููู" and news['category'] != chosen_category:
                continue
            
            filtered_news.append(news)
        
        return filtered_news[:10]  # ุฃูู 10 ุฃุฎุจุงุฑ
        
    except Exception as e:
        st.error(f"ุฎุทุฃ ูู ุฌูุจ ุงูุฃุฎุจุงุฑ ูู {source_name}: {str(e)}")
        return []

def smart_news_fetcher(source_name, source_info, keywords, date_from, date_to, chosen_category):
    """ุฌุงูุจ ุงูุฃุฎุจุงุฑ ุงูุฐูู - ูุฌุฑุจ ุนุฏุฉ ุทุฑู"""
    all_news = []
    
    # ุงููุญุงููุฉ ุงูุฃููู: RSS
    if source_info.get("rss_options"):
        st.info("๐ ุงููุญุงููุฉ ุงูุฃููู: ุงูุจุญุซ ุนู RSS...")
        for rss_url in source_info["rss_options"]:
            try:
                news = fetch_rss_news(source_name, rss_url, keywords, date_from, date_to, chosen_category)
                if news:
                    st.success(f"โ ุชู ุงูุนุซูุฑ ุนูู {len(news)} ุฎุจุฑ ูู RSS: {rss_url}")
                    all_news.extend(news)
                    break
            except:
                continue
    
    # ุงููุญุงููุฉ ุงูุซุงููุฉ: ุชุญููู ุงููููุน ูุจุงุดุฑุฉ
    if not all_news:
        st.info("๐ ุงููุญุงููุฉ ุงูุซุงููุฉ: ุชุญููู ุงููููุน ูุจุงุดุฑุฉ...")
        website_news = fetch_website_news(source_name, source_info["url"], keywords, date_from, date_to, chosen_category)
        if website_news:
            st.success(f"โ ุชู ุงุณุชุฎุฑุงุฌ {len(website_news)} ุฎุจุฑ ูู ุงููููุน ูุจุงุดุฑุฉ")
            all_news.extend(website_news)
    
    # ุฅุฒุงูุฉ ุงูููุฑุฑ
    seen_titles = set()
    unique_news = []
    for news in all_news:
        if news['title'] not in seen_titles:
            seen_titles.add(news['title'])
            unique_news.append(news)
    
    return unique_news

def export_to_word(news_list):
    doc = Document()
    doc.add_heading('ุชูุฑูุฑ ุงูุฃุฎุจุงุฑ ุงููุฌูุนุฉ', 0)
    doc.add_paragraph(f'ุชุงุฑูุฎ ุงูุชูุฑูุฑ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    doc.add_paragraph(f'ุนุฏุฏ ุงูุฃุฎุจุงุฑ: {len(news_list)}')
    doc.add_paragraph('---')
    
    for i, news in enumerate(news_list, 1):
        doc.add_heading(f'{i}. {news["title"]}', level=2)
        doc.add_paragraph(f"ุงููุตุฏุฑ: {news['source']}")
        doc.add_paragraph(f"ุงูุชุตููู: {news['category']}")
        doc.add_paragraph(f"ุงูุชุงุฑูุฎ: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ุทุฑููุฉ ุงูุงุณุชุฎุฑุงุฌ: {news.get('extraction_method', 'ุบูุฑ ูุญุฏุฏ')}")
        doc.add_paragraph(f"ุงูุชุญููู ุงูุนุงุทูู: {news['sentiment']}")
        doc.add_paragraph(f"ุงูููุฎุต: {news['summary']}")
        doc.add_paragraph(f"ุงูุฑุงุจุท: {news['link']}")
        doc.add_paragraph('---')
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    # ุชุฑุชูุจ ุงูุฃุนูุฏุฉ
    columns_order = ['source', 'title', 'category', 'sentiment', 'published', 'summary', 'link', 'extraction_method']
    df = df.reindex(columns=[col for col in columns_order if col in df.columns])
    
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ุงูุฃุฎุจุงุฑ')
    buffer.seek(0)
    return buffer

# ูุตุงุฏุฑ ุงูุฃุฎุจุงุฑ ุงููุญุณููุฉ
general_rss_feeds = {
    "BBC ุนุฑุจู": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "ุงูุฌุฒูุฑุฉ": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 ุนุฑุจู": "https://www.france24.com/ar/rss",
    "ุณูุงู ูููุฒ ุนุฑุจูุฉ": "https://www.skynewsarabia.com/web/rss",
    "ุนุฑุจู21": "https://arabi21.com/feed"
}

iraqi_news_sources = {
    "ูุฒุงุฑุฉ ุงูุฏุงุฎููุฉ ุงูุนุฑุงููุฉ": {
        "url": "https://moi.gov.iq/",
        "type": "website",
        "rss_options": [
            "https://moi.gov.iq/feed/",
            "https://moi.gov.iq/rss.xml"
        ]
    },
    "ูุฐุง ุงูููู": {
        "url": "https://hathalyoum.net/",
        "type": "website",
        "rss_options": [
            "https://hathalyoum.net/feed/",
            "https://hathalyoum.net/rss.xml"
        ]
    },
    "ุงูุนุฑุงู ุงูููู": {
        "url": "https://iraqtoday.com/",
        "type": "website",
        "rss_options": [
            "https://iraqtoday.com/feed/",
            "https://iraqtoday.com/rss.xml"
        ]
    },
    "ุฑุฆุงุณุฉ ุงูุฌูููุฑูุฉ ุงูุนุฑุงููุฉ": {
        "url": "https://presidency.iq/default.aspx",
        "type": "website",
        "rss_options": [
            "https://presidency.iq/feed/",
            "https://presidency.iq/rss.xml"
        ]
    },
    "ุงูุดุฑู ุงูุฃูุณุท": {
        "url": "https://asharq.com/",
        "type": "website",
        "rss_options": [
            "https://asharq.com/feed/",
            "https://asharq.com/rss.xml"
        ]
    },
    "RT Arabic - ุงูุนุฑุงู": {
        "url": "https://arabic.rt.com/focuses/10744-%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss_options": [
            "https://arabic.rt.com/rss/"
        ]
    },
    "ุฅูุฏุจูุฏูุช ุนุฑุจูุฉ": {
        "url": "https://www.independentarabia.com/",
        "type": "website",
        "rss_options": [
            "https://www.independentarabia.com/rss"
        ]
    },
    "ูุฑุงูุณ 24 ุนุฑุจู": {
        "url": "https://www.france24.com/ar/",
        "type": "website",
        "rss_options": [
            "https://www.france24.com/ar/rss"
        ]
    }
}

# ูุงุฌูุฉ ุงููุณุชุฎุฏู ุงููุญุณููุฉ
st.sidebar.header("โ๏ธ ุฅุนุฏุงุฏุงุช ุงูุจุญุซ ุงููุชูุฏู")

# ุงุฎุชูุงุฑ ููุน ุงููุตุฏุฑ
source_type = st.sidebar.selectbox(
    "๐ ุงุฎุชุฑ ููุน ุงููุตุฏุฑ:",
    ["ุงููุตุงุฏุฑ ุงูุนุงูุฉ", "ุงููุตุงุฏุฑ ุงูุนุฑุงููุฉ"],
    help="ุงููุตุงุฏุฑ ุงูุนุงูุฉ ุชุนุชูุฏ ุนูู RSSุ ุงููุตุงุฏุฑ ุงูุนุฑุงููุฉ ุชุณุชุฎุฏู ุชูููุงุช ูุชูุฏูุฉ"
)

if source_type == "ุงููุตุงุฏุฑ ุงูุนุงูุฉ":
    selected_source = st.sidebar.selectbox("๐ ุงุฎุชุฑ ูุตุฏุฑ ุงูุฃุฎุจุงุฑ:", list(general_rss_feeds.keys()))
    source_url = general_rss_feeds[selected_source]
    source_info = {"type": "rss", "url": source_url}
else:
    selected_source = st.sidebar.selectbox("๐ฎ๐ถ ุงุฎุชุฑ ูุตุฏุฑ ุงูุฃุฎุจุงุฑ ุงูุนุฑุงูู:", list(iraqi_news_sources.keys()))
    source_info = iraqi_news_sources[selected_source]

# ุฅุนุฏุงุฏุงุช ุงูุจุญุซ ุงููุญุณููุฉ
st.sidebar.markdown("### ๐ ุงูุจุญุซ ุงูููุชูุญ ุจุงููููุงุช ุงูููุชุงุญูุฉ")
keywords_input = st.sidebar.text_area(
    "ูููุงุช ููุชุงุญูุฉ ููุจุญุซ (ุฃู ูููุฉ ุฃู ุนุจุงุฑุฉ):", 
    value="",
    height=100,
    help="""
    โจ ููููู ุงูุจุญุซ ุจุฃู ูููุฉ ุฃู ุนุจุงุฑุฉ:
    
    ุฃูุซูุฉ ููุจุญุซ:
    โข ูููุฉ ูุงุญุฏุฉ: ุจุบุฏุงุฏ
    โข ุนุฏุฉ ูููุงุช: ุงูุฑุฆูุณุ ุงููุฒูุฑุ ุงูุญูููุฉ
    โข ุนุจุงุฑุงุช: ุงูุดุฑู ุงูุฃูุณุทุ ูุฑุฉ ุงููุฏู
    โข ุฃู ููุถูุน: ููุฑููุงุ ุงูุชุตุงุฏุ ุชุนููู
    
    ๐ก ููุงุญุธุงุช:
    - ุงุชุฑูู ูุงุฑุบุงู ูุนุฑุถ ุฌููุน ุงูุฃุฎุจุงุฑ
    - ุงุณุชุฎุฏู ุงููุงุตูุฉ ูููุตู ุจูู ุงููููุงุช
    - ุงูุจุญุซ ูุดูู ุงูุนูุงููู ูุงูููุฎุตุงุช
    - ูุฏุนู ุงูุจุญุซ ุงูุฌุฒุฆู ูุงููุฑุงุฏูุงุช
    """
)

# ูุนุงูุฌุฉ ุงููููุงุช ุงูููุชุงุญูุฉ
keywords = []
if keywords_input.strip():
    # ุชูุณูู ุงููุต ุฅูู ูููุงุช ููุชุงุญูุฉ
    raw_keywords = [kw.strip() for kw in keywords_input.split(",")]
    # ุฅุฒุงูุฉ ุงููููุงุช ุงููุงุฑุบุฉ
    keywords = [kw for kw in raw_keywords if kw]

# ุนุฑุถ ุงููููุงุช ุงูููุชุงุญูุฉ ุงูููุฏุฎูุฉ
if keywords:
    st.sidebar.success(f"โ ุชู ุชุญุฏูุฏ {len(keywords)} ูููุฉ ููุชุงุญูุฉ ููุจุญุซ")
    with st.sidebar.expander("๐ ุงููููุงุช ุงูููุชุงุญูุฉ ุงููุญุฏุฏุฉ"):
        for i, keyword in enumerate(keywords, 1):
            st.write(f"{i}. **{keyword}**")
else:
    st.sidebar.info("๐ ุณูุชู ุนุฑุถ ุฌููุน ุงูุฃุฎุจุงุฑ (ุจุฏูู ููุชุฑุฉ)")

category_filter = st.sidebar.selectbox(
    "๐ ุงุฎุชุฑ ุงูุชุตููู:", 
    ["ุงููู"] + list(category_keywords.keys()),
    help="ููุชุฑุฉ ุงูุฃุฎุจุงุฑ ุญุณุจ ุงูุชุตููู ุงูุชููุงุฆู"
)

# ุฅุนุฏุงุฏุงุช ุงูุชุงุฑูุฎ
col_date1, col_date2 = st.sidebar.columns(2)
with col_date1:
    date_from = st.date_input("๐ ูู ุชุงุฑูุฎ:", datetime.today() - timedelta(days=7))
with col_date2:
    date_to = st.date_input("๐ ุฅูู ุชุงุฑูุฎ:", datetime.today())

# ุฎูุงุฑุงุช ูุชูุฏูุฉ
with st.sidebar.expander("โ๏ธ ุฎูุงุฑุงุช ูุชูุฏูุฉ"):
    max_news = st.slider("ุนุฏุฏ ุงูุฃุฎุจุงุฑ ุงูุฃูุตู:", 5, 50, 20)
    include_sentiment = st.checkbox("ุชุญููู ุงููุดุงุนุฑ", True)
    include_categorization = st.checkbox("ุงูุชุตููู ุงูุชููุงุฆู", True)
    
    # ุฎูุงุฑุงุช ุงูุจุญุซ ุงููุชูุฏู
    st.markdown("#### ๐ ุฎูุงุฑุงุช ุงูุจุญุซ ุงููุชูุฏู")
    search_in_title = st.checkbox("ุงูุจุญุซ ูู ุงูุนูุงููู", True)
    search_in_summary = st.checkbox("ุงูุจุญุซ ูู ุงูููุฎุตุงุช", True)
    use_partial_search = st.checkbox("ุงูุจุญุซ ุงูุฌุฒุฆู", True, help="ุงูุจุญุซ ุนู ุฃุฌุฒุงุก ูู ุงููููุงุช")
    use_synonyms = st.checkbox("ุงุณุชุฎุฏุงู ุงููุฑุงุฏูุงุช", True, help="ุงูุจุญุซ ูู ุงููุฑุงุฏูุงุช ูุงููููุงุช ุฐุงุช ุงูุตูุฉ")

run = st.sidebar.button("๐ฅ ุฌูุจ ุงูุฃุฎุจุงุฑ", type="primary", help="ุงุจุฏุฃ ุนูููุฉ ุฌูุจ ูุชุญููู ุงูุฃุฎุจุงุฑ")

# ูุนูููุงุช ุฅุถุงููุฉ ูู ุงูุดุฑูุท ุงูุฌุงูุจู
st.sidebar.markdown("---")
st.sidebar.markdown("### ๐ก ูุตุงุฆุญ ููุจุญุซ ุงููุนูุงู")
st.sidebar.markdown("""
**๐ฏ ุฃูุซูุฉ ููุจุญุซ:**
- `ุจุบุฏุงุฏุ ุงูุจุตุฑุฉุ ุฃุฑุจูู` - ูุฏู ุนุฑุงููุฉ
- `ููุฑููุงุ ุตุญุฉุ ููุงุญ` - ููุงุถูุน ุตุญูุฉ  
- `ููุทุ ุงูุชุตุงุฏุ ุงุณุชุซูุงุฑ` - ููุงุถูุน ุงูุชุตุงุฏูุฉ
- `ุงูุชุฎุงุจุงุชุ ุจุฑููุงูุ ุญูููุฉ` - ููุงุถูุน ุณูุงุณูุฉ

**โจ ููุฒุงุช ุงูุจุญุซ:**
- ุงูุจุญุซ ูู ุฃู ูููุฉ ุฃู ุนุจุงุฑุฉ
- ูุง ุญุงุฌุฉ ููุงูุชุฒุงู ุจูููุงุช ูุญุฏุฏุฉ
- ุงูุจุญุซ ุงูุฐูู ุจุงููุฑุงุฏูุงุช
- ุงูุจุญุซ ุงูุฌุฒุฆู ูููููุงุช ุงูุทูููุฉ
""")

# ุนุฑุถ ุงููุชุงุฆุฌ
if run:
    # ุงูุชุญูู ูู ุตุญุฉ ุงูุจูุงูุงุช ุงูููุฏุฎูุฉ
    if date_from > date_to:
        st.error("โ ุชุงุฑูุฎ ุงูุจุฏุงูุฉ ูุฌุจ ุฃู ูููู ูุจู ุชุงุฑูุฎ ุงูููุงูุฉ")
        st.stop()
    
    # ุนุฑุถ ูุนูููุงุช ุงูุจุญุซ
    st.info(f"""
    ๐ **ูุนูููุงุช ุงูุจุญุซ:**
    - ุงููุตุฏุฑ: **{selected_source}**
    - ุงููููุงุช ุงูููุชุงุญูุฉ: **{len(keywords)}** {"ูููุฉ" if keywords else "ุจุฏูู ููุชุฑุฉ"}
    - ุงูุชุตููู: **{category_filter}**
    - ุงููุชุฑุฉ ุงูุฒูููุฉ: **{date_from}** ุฅูู **{date_to}**
    - ุนุฏุฏ ุงูุฃุฎุจุงุฑ ุงููุทููุจ: **{max_news}** ุฎุจุฑ ูุญุฏ ุฃูุตู
    """)
    
    if keywords:
        st.success(f"๐ฏ ุงูุจุญุซ ุนู: {', '.join(keywords)}")
    
    with st.spinner("๐ค ุฌุงุฑู ุชุดุบูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุฌูุจ ุงูุฃุฎุจุงุฑ..."):
        start_time = time.time()
        
        if source_type == "ุงููุตุงุฏุฑ ุงูุนุงูุฉ":
            news = fetch_rss_news(
                selected_source,
                source_info["url"],
                keywords,
                date_from,
                date_to,
                category_filter
            )
        else:
            news = smart_news_fetcher(
                selected_source,
                source_info,
                keywords,
                date_from,
                date_to,
                category_filter
            )
        
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
    
    if news:
        st.success(f"๐ ุชู ุฌูุจ {len(news)} ุฎุจุฑ ูู {selected_source} ูู {processing_time} ุซุงููุฉ")
        
        # ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ ููุญุณูุฉ
        st.subheader("๐ ุฅุญุตุงุฆูุงุช ุงูุฃุฎุจุงุฑ ุงููุฌูุนุฉ")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("๐ฐ ุฅุฌูุงูู ุงูุฃุฎุจุงุฑ", len(news))
        with col2:
            categories = [n['category'] for n in news]
            most_common_cat = Counter(categories).most_common(1)[0][0] if categories else "ุบูุฑ ูุญุฏุฏ"
            st.metric("๐ ุฃูุซุฑ ุชุตููู", most_common_cat)
        with col3:
            positive_news = len([n for n in news if "ุฅูุฌุงุจู" in n['sentiment']])
            st.metric("๐ ุฃุฎุจุงุฑ ุฅูุฌุงุจูุฉ", positive_news)
        with col4:
            negative_news = len([n for n in news if "ุณูุจู" in n['sentiment']])
            st.metric("๐ ุฃุฎุจุงุฑ ุณูุจูุฉ", negative_news)
        with col5:
            st.metric("โฑ๏ธ ููุช ุงููุนุงูุฌุฉ", f"{processing_time}s")
        
        # ุนุฑุถ ุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู ุจุดูู ุจุงุฑุฒ
        st.subheader("๐ค ุงููููุงุช ุงูุฃูุซุฑ ุชูุฑุงุฑุงู ูู ุงูุฃุฎุจุงุฑ")
        all_text = " ".join([n['title'] + " " + n['summary'] for n in news])
        # ุชูุธูู ุงููุต ูุชุญุณูู ุงูุจุญุซ
        words = re.findall(r'\b[ุฃ-ู]{2,}\b', all_text)  # ูููุงุช ุนุฑุจูุฉ ูู ุญุฑููู ูุฃูุซุฑ
        word_freq = Counter(words).most_common(20)  # ุฃูู 20 ูููุฉ
        
        if word_freq:
            # ุนุฑุถ ุงููููุงุช ูู ุดูู ุฌุฏูู ููุธู
            col_words1, col_words2 = st.columns(2)
            
            with col_words1:
                st.markdown("**ุงููููุงุช ุงูุฃูุซุฑ ุชูุฑุงุฑุงู (1-10):**")
                for i, (word, freq) in enumerate(word_freq[:10], 1):
                    percentage = (freq / len(news)) * 100
                    st.write(f"{i}. **{word}**: {freq} ูุฑุฉ ({percentage:.1f}% ูู ุงูุฃุฎุจุงุฑ)")
            
            with col_words2:
                st.markdown("**ุงููููุงุช ุงูุฃูุซุฑ ุชูุฑุงุฑุงู (11-20):**")
                for i, (word, freq) in enumerate(word_freq[10:20], 11):
                    percentage = (freq / len(news)) * 100
                    st.write(f"{i}. **{word}**: {freq} ูุฑุฉ ({percentage:.1f}% ูู ุงูุฃุฎุจุงุฑ)")
        
        # ุฅุถุงูุฉ ูุฑุจุน ูุนูููุงุช ุนู ุงูุชุญููู
        st.info(f"""
        ๐ **ููุฎุต ุงูุชุญููู:**
        - ุชู ุชุญููู **{len(news)}** ุฎุจุฑ ูู ูุตุฏุฑ **{selected_source}**
        - ุชู ุงุณุชุฎุฑุงุฌ **{len(word_freq)}** ูููุฉ ูุฎุชููุฉ
        - ุฃูุซุฑ ูููุฉ ุชูุฑุงุฑุงู: **{word_freq[0][0]}** ({word_freq[0][1]} ูุฑุฉ) {f"ุฅุฐุง ูุงูุช ูุชููุฑุฉ" if word_freq else ""}
        - ุงูุชุตููู ุงูุฃูุซุฑ ุดููุนุงู: **{most_common_cat}**
        - ูุณุจุฉ ุงูุฃุฎุจุงุฑ ุงูุฅูุฌุงุจูุฉ: **{(positive_news/len(news)*100):.1f}%**
        - ุงูุจุญุซ ุงููุณุชุฎุฏู: **{"ููุชูุญ" if keywords else "ุดุงูู"}** {"(" + ", ".join(keywords[:3]) + "...)" if len(keywords) > 3 else "(" + ", ".join(keywords) + ")" if keywords else ""}
        """)
        
        st.markdown("---")
        
        # ุนุฑุถ ุงูุฃุฎุจุงุฑ ุจุชุตููู ูุญุณู
        st.subheader("๐ ุงูุฃุฎุจุงุฑ ุงููุฌูุนุฉ")
        
        for i, item in enumerate(news[:max_news], 1):
            # ุญุงููุฉ ุฑุฆูุณูุฉ ูุน ุชุตููู ุฃููู
            with st.container():
                # ุฅูุดุงุก ุฃุนูุฏุฉ ููุชุฎุทูุท
                if item.get('image'):
                    col_image, col_content = st.columns([1, 5])  # ุนููุฏ ุฃุตุบุฑ ููุตูุฑุฉุ ุฃูุจุฑ ูููุญุชูู
                    
                    with col_image:
                        st.image(
                            item['image'], 
                            width=80,  # ุชุตุบูุฑ ุฃูุซุฑ ููุตูุฑุฉ
                            caption="",
                            use_column_width=False
                        )
                    
                    with col_content:
                        # ุงูุนููุงู
                        st.markdown(f"### ๐ฐ {item['title']}")
                        
                        # ูุนูููุงุช ุณุฑูุนุฉ ูู ุตู ูุงุญุฏ
                        info_col1, info_col2, info_col3, info_col4 = st.columns(4)
                        with info_col1:
                            st.markdown(f"**๐ข {item['source']}**")
                        with info_col2:
                            st.markdown(f"**๐ {item['category']}**")
                        with info_col3:
                            st.markdown(f"**๐ญ {item['sentiment']}**")
                        with info_col4:
                            st.markdown(f"**๐ {item['published'].strftime('%m-%d %H:%M')}**")
                        
                        # ุงูููุฎุต ุงููุญุณู
                        st.markdown("**๐ ุงูููุฎุต ุงูุชูุตููู:**")
                        st.markdown(f">{item['summary']}")
                        
                        # ุงูุฑุงุจุท
                        st.markdown(f"๐ **[ูุฑุงุกุฉ ุงูููุงู ูุงููุงู โ]({item['link']})**")
                
                else:
                    # ุชุฎุทูุท ุจุฏูู ุตูุฑุฉ
                    st.markdown(f"### ๐ฐ {item['title']}")
                    
                    # ูุนูููุงุช ุณุฑูุนุฉ
                    info_col1, info_col2, info_col3, info_col4, info_col5 = st.columns(5)
                    with info_col1:
                        st.markdown(f"**๐ข {item['source']}**")
                    with info_col2:
                        st.markdown(f"**๐ {item['category']}**")
                    with info_col3:
                        st.markdown(f"**๐ญ {item['sentiment']}**")
                    with info_col4:
                        st.markdown(f"**๐ {item['published'].strftime('%Y-%m-%d')}**")
                    with info_col5:
                        st.markdown(f"**๐ง {item.get('extraction_method', 'ุบูุฑ ูุญุฏุฏ')}**")
                    
                    # ุงูููุฎุต ูู ูุฑุจุน ูููุตู
                    st.markdown("**๐ ุงูููุฎุต ุงูุชูุตููู:**")
                    st.info(item['summary'])
                    
                    # ุงูุฑุงุจุท
                    st.markdown(f"๐ **[ูุฑุงุกุฉ ุงูููุงู ูุงููุงู โ]({item['link']})**")
                
                # ุฅุจุฑุงุฒ ุงููููุงุช ุงูููุชุงุญูุฉ ุงูููุฌูุฏุฉ ูู ุงูุฎุจุฑ
                if keywords:
                    found_keywords = []
                    full_text = (item['title'] + " " + item['summary']).lower()
                    for keyword in keywords:
                        if keyword.lower() in full_text:
                            found_keywords.append(keyword)
                    
                    if found_keywords:
                        st.markdown(f"๐ฏ **ุงููููุงุช ุงููุทุงุจูุฉ:** {', '.join(found_keywords)}")
                
                # ุฎุท ูุงุตู ุฃููู
                st.markdown("---")
        
        # ุชุตุฏูุฑ ุงูุจูุงูุงุช
        st.subheader("๐ค ุชุตุฏูุฑ ุงูุจูุงูุงุช")
        col_export1, col_export2, col_export3 = st.columns(3)
        
        with col_export1:
            word_file = export_to_word(news)
            st.download_button(
                "๐ ุชุญููู Word",
                data=word_file,
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        
        with col_export2:
            excel_file = export_to_excel(news)
            st.download_button(
                "๐ ุชุญููู Excel",
                data=excel_file,
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        with col_export3:
            json_data = json.dumps(news, ensure_ascii=False, default=str, indent=2)
            st.download_button(
                "๐พ ุชุญููู JSON",
                data=json_data.encode('utf-8'),
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                mime="application/json"
            )
        
        # ุชุญูููุงุช ูุชูุฏูุฉ
        with st.expander("๐ ุชุญูููุงุช ูุชูุฏูุฉ"):
            col_analysis1, col_analysis2 = st.columns(2)
            
            with col_analysis1:
                st.subheader("๐ ุชูุฒูุน ุงูุชุตูููุงุช")
                categories = [n['category'] for n in news]
                category_counts = Counter(categories)
                
                for cat, count in category_counts.most_common():
                    percentage = (count / len(news)) * 100
                    st.write(f"โข **{cat}**: {count} ({percentage:.1f}%)")
            
            with col_analysis2:
                st.subheader("๐ญ ุชุญููู ุงููุดุงุนุฑ")
                sentiments = [n['sentiment'] for n in news]
                sentiment_counts = Counter(sentiments)
                
                for sent, count in sentiment_counts.items():
                    percentage = (count / len(news)) * 100
                    st.write(f"โข **{sent}**: {count} ({percentage:.1f}%)")
            
            st.subheader("๐ค ุชูุตูู ุงููููุงุช ุงููุชูุฑุฑุฉ")
            all_text = " ".join([n['title'] + " " + n['summary'] for n in news])
            # ุชูุธูู ุงููุต
            words = re.findall(r'\b[ุฃ-ู]{2,}\b', all_text)  # ูููุงุช ุนุฑุจูุฉ ูู ุญุฑููู ูุฃูุซุฑ
            word_freq = Counter(words).most_common(30)  # ุฃูู 30 ูููุฉ
            
            if word_freq:
                st.markdown("**ุฃูู 30 ูููุฉ ูู ุงูุฃุฎุจุงุฑ:**")
                cols = st.columns(3)
                for i, (word, freq) in enumerate(word_freq):
                    with cols[i % 3]:
                        percentage = (freq / len(news)) * 100
                        st.write(f"**{word}**: {freq} ูุฑุฉ ({percentage:.1f}%)")
            
            # ุชุญููู ุงููููุงุช ุงูููุชุงุญูุฉ ุงููุทุงุจูุฉ
            if keywords:
                st.subheader("๐ฏ ุชุญููู ูุทุงุจูุฉ ุงููููุงุช ุงูููุชุงุญูุฉ")
                keyword_matches = {}
                
                for news_item in news:
                    full_text = (news_item['title'] + " " + news_item['summary']).lower()
                    for keyword in keywords:
                        if keyword.lower() in full_text:
                            if keyword not in keyword_matches:
                                keyword_matches[keyword] = 0
                            keyword_matches[keyword] += 1
                
                if keyword_matches:
                    st.markdown("**ูุนุฏู ูุทุงุจูุฉ ุงููููุงุช ุงูููุชุงุญูุฉ:**")
                    for keyword, count in sorted(keyword_matches.items(), key=lambda x: x[1], reverse=True):
                        percentage = (count / len(news)) * 100
                        st.write(f"โข **{keyword}**: {count} ุฃุฎุจุงุฑ ({percentage:.1f}%)")
                else:
                    st.warning("ูู ุชูุทุงุจู ุฃู ูู ุงููููุงุช ุงูููุชุงุญูุฉ ุงููุญุฏุฏุฉ ูู ุงูุฃุฎุจุงุฑ ุงูููุฌูุนุฉ")
    
    else:
        st.warning("โ ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃุฎุจุงุฑ ุจุงูุดุฑูุท ุงููุญุฏุฏุฉ")
        
        # ุงูุชุฑุงุญุงุช ูููุณุงุนุฏุฉ
        st.markdown("### ๐ก ุงูุชุฑุงุญุงุช ูุชุญุณูู ุงูุจุญุซ:")
        suggestions = [
            "๐ **ูุณุน ูุทุงู ุงูุจุญุซ**: ุฌุฑุจ ูููุงุช ููุชุงุญูุฉ ุฃูุซุฑ ุนููููุฉ",
            "๐ **ูุณุน ุงููุชุฑุฉ ุงูุฒูููุฉ**: ุงุฎุชุฑ ูุชุฑุฉ ุฒูููุฉ ุฃุทูู",
            "๐ **ุบูุฑ ุงููุตุฏุฑ**: ุฌุฑุจ ูุตุฏุฑ ุฃุฎุจุงุฑ ูุฎุชูู",
            "๐ **ุบูุฑ ุงูุชุตููู**: ุงุฎุชุฑ 'ุงููู' ุจุฏูุงู ูู ุชุตููู ูุญุฏุฏ",
            "๐ค **ุชุจุณูุท ุงููููุงุช**: ุงุณุชุฎุฏู ูููุงุช ุฃุจุณุท ูุฃูุซุฑ ุดููุนุงู"
        ]
        
        for suggestion in suggestions:
            st.markdown(f"- {suggestion}")
        
        if keywords:
            st.info(f"๐ญ **ุชู ุงูุจุญุซ ุนู:** {', '.join(keywords)}")
            st.markdown("ุฌุฑุจ ูููุงุช ููุชุงุญูุฉ ูุฎุชููุฉ ุฃู ุงุชุฑููุง ูุงุฑุบุฉ ูุนุฑุถ ุฌููุน ุงูุฃุฎุจุงุฑ")
        
        st.markdown(f"๐ **[ุฒูุงุฑุฉ {selected_source} ูุจุงุดุฑุฉ]({source_info['url']})**")

# ูุนูููุงุช ูู ุงูุดุฑูุท ุงูุฌุงูุจู
st.sidebar.markdown("---")
st.sidebar.success("โ **ุงูุจุญุซ ุงูููุชูุญ ูุชุงุญ ุงูุขู!**")
st.sidebar.info("""
๐ **ููุฒุงุช ุงูุจุญุซ ุงูุฌุฏูุฏุฉ:**
- ๐ ุจุญุซ ููุชูุญ ุจุฃู ูููุฉ
- ๐ ูุง ุญุงุฌุฉ ููููุงุช ูุญุฏุฏุฉ ูุณุจูุงู  
- ๐ง ุจุญุซ ุฐูู ุจุงููุฑุงุฏูุงุช
- ๐ค ุจุญุซ ุฌุฒุฆู ูููููุงุช
- ๐ฏ ุฅุจุฑุงุฒ ุงููููุงุช ุงููุทุงุจูุฉ
- ๐ ุชุญููู ูุทุงุจูุฉ ูุชูุฏู

**ุชูููุงุช ูุชูุฏูุฉ:**
- ุฌูุจ RSS ุชููุงุฆู
- ุชุญููู ููุงูุน ุงูููุจ
- ุชุตููู ุฐูู ููุฃุฎุจุงุฑ
- ุชุญููู ุงููุดุงุนุฑ
- ุฅุฒุงูุฉ ุงููุญุชูู ุงูููุฑุฑ
- ููุฎุตุงุช ุฐููุฉ ูุทููุฉ
""")

# ูุนูููุงุช ุชูููุฉ ูุญุฏุซุฉ
with st.expander("โน๏ธ ูุนูููุงุช ุชูููุฉ - ุชุญุฏูุซ ุงูุจุญุซ ุงูููุชูุญ"):
    st.markdown("""
    ### ๐ **ุฃุญุฏุซ ุงูุชุญุณููุงุช - ุงูุจุญุซ ุงูููุชูุญ:**
    
    #### ๐ **ุงูุจุญุซ ุงูููุชูุญ ุงูุฌุฏูุฏ:**
    - **ุญุฑูุฉ ูุงููุฉ ูู ุงูุจุญุซ**: ููููู ุงูุจุญุซ ุจุฃู ูููุฉ ุฃู ุนุจุงุฑุฉ
    - **ูุง ูููุฏ ุนูู ุงููููุงุช**: ุบูุฑ ูุญุฏูุฏ ุจูุงููุณ ูููุงุช ูุญุฏุฏ ูุณุจูุงู
    - **ุจุญุซ ูุชุนุฏุฏ ุงูุทุจูุงุช**: ูุจุญุซ ูู ุงูุนูุงููู ูุงูููุฎุตุงุช ูุนุงู
    - **ูุฑููุฉ ูู ุงูุฅุฏุฎุงู**: ุฏุนู ุงููููุงุช ุงูููุฑุฏุฉ ูุงูุนุจุงุฑุงุช ุงููุฑูุจุฉ
    
    #### ๐ง **ุฎูุงุฑุฒููุงุช ุงูุจุญุซ ุงูุฐููุฉ:**
    - **`flexible_keyword_search()`**: ุจุญุซ ูุจุงุดุฑ ููุฑู
    - **`enhanced_keyword_search()`**: ุจุญุซ ูุชูุฏู ุจุงููุฑุงุฏูุงุช
    - **ุงูุจุญุซ ุงูุฌุฒุฆู**: ุงูุนุซูุฑ ุนูู ุฃุฌุฒุงุก ุงููููุงุช ุงูุทูููุฉ
    - **ุงูุจุญุซ ุจุงูุฌุฐูุฑ**: ุชุญููู ุงูุฌุฐูุฑ ุงูุนุฑุจูุฉ ุงูุชูุฑูุจู
    - **ุฏุนู ุงููุฑุงุฏูุงุช**: ูุงููุณ ูุฑุงุฏูุงุช ูุงุจู ููุชูุณูุน
    
    #### ๐ **ููุฒุงุช ุงููุงุฌูุฉ ุงููุญุณููุฉ:**
    - **ูุฑุจุน ูุต ูุจูุฑ**: ูุณูููุฉ ุฅุฏุฎุงู ุงููููุงุช ุงููุชุนุฏุฏุฉ
    - **ุนุฑุถ ุงููููุงุช ุงููุญุฏุฏุฉ**: ูุงุฆูุฉ ุจุงููููุงุช ุงูููุชุงุญูุฉ ุงูููุฏุฎูุฉ
    - **ุฅุจุฑุงุฒ ุงููุทุงุจูุงุช**: ุนุฑุถ ุงููููุงุช ุงููุทุงุจูุฉ ููู ุฎุจุฑ
    - **ุชุญููู ูุชูุฏู**: ุฅุญุตุงุฆูุงุช ูุทุงุจูุฉ ุงููููุงุช ุงูููุชุงุญูุฉ
    - **ูุตุงุฆุญ ุชูุงุนููุฉ**: ุฃูุซูุฉ ูุฅุฑุดุงุฏุงุช ููุจุญุซ ุงููุนูุงู
    
    #### ๐ฏ **ุฃูุซูุฉ ููุงุณุชุฎุฏุงู:**
    
    **ุงูุจุญุซ ุงูุจุณูุท:**
    ```
    ุจุบุฏุงุฏ
    ููุฑููุง
    ุงูุชุตุงุฏ
    ```
    
    **ุงูุจุญุซ ุงููุชุนุฏุฏ:**
    ```
    ุงูุฑุฆูุณุ ุงููุฒูุฑุ ุงูุญูููุฉ
    ูุฑุฉ ุงููุฏูุ ุฑูุงุถุฉุ ุจุทููุฉ
    ```
    
    **ุงูุจุญุซ ุจุงูุนุจุงุฑุงุช:**
    ```
    ุงูุดุฑู ุงูุฃูุณุท
    ูุฒุงุฑุฉ ุงูุฏุงุฎููุฉ
    ูุฃุณ ุงูุนุงูู
    ```
    
    **ุงูุจุญุซ ุงูููุถูุนู:**
    ```
    ุชุนูููุ ุฌุงูุนุฉุ ุทูุงุจ
    ุตุญุฉุ ูุณุชุดููุ ููุงุญ
    ุชูููููุฌูุงุ ุฐูุงุก ุงุตุทูุงุนู
    ```
    
    #### ๐ง **ุงูุชุญุณููุงุช ุงูุชูููุฉ:**
    
    **ูุนุงูุฌุฉ ุงููุตูุต:**
    - ุชูุธูู ุงููููุงุช ุงูููุชุงุญูุฉ ูู ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    - ุชุญููู ุงูุฃุญุฑู ููุญุงูุฉ ุงูุตุบูุฑุฉ ููููุงุฑูุฉ
    - ุฏุนู ุงูุจุญุซ ูู ุงููุตูุต ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
    
    **ุฎูุงุฑุฒููุงุช ุงูุจุญุซ:**
    ```python
    def enhanced_keyword_search(text, keywords):
        # ุจุญุซ ูุจุงุดุฑ
        if keyword in text_lower:
            return True
        
        # ุจุญุซ ุจุงููุฑุงุฏูุงุช
        for synonym in synonyms:
            if synonym in text_lower:
                return True
        
        # ุจุญุซ ุฌุฒุฆู ูููููุงุช ุงูุทูููุฉ
        if len(keyword) > 4:
            parts = [keyword[i:i+4] for i in range(len(keyword)-3)]
            for part in parts:
                if part in text_lower:
                    return True
    ```
    
    **ูุงููุณ ุงููุฑุงุฏูุงุช ุงููุงุจู ููุชูุณูุน:**
    ```python
    synonyms_dict = {
        "ุญุฑุจ": ["ูุชุงู", "ูุนุฑูุฉ", "ุตุฑุงุน", "ูุฒุงุน"],
        "ุงูุชุตุงุฏ": ["ูุงู", "ุชุฌุงุฑุฉ", "ุงุณุชุซูุงุฑ", "ุจูุฑุตุฉ"],
        "ุณูุงุณุฉ": ["ุญูููุฉ", "ุฏููุฉ", "ุฑุฆูุณ", "ูุฒูุฑ"],
        # ูููู ุฅุถุงูุฉ ุงููุฒูุฏ...
    }
    ```
    
    #### ๐ **ูููุฒุงุช ุงูุชุญููู ุงูุฌุฏูุฏุฉ:**
    - **ุฅุญุตุงุฆูุงุช ุงููุทุงุจูุฉ**: ูุณุจุฉ ูุทุงุจูุฉ ูู ูููุฉ ููุชุงุญูุฉ
    - **ุนุฑุถ ุงููููุงุช ุงููุทุงุจูุฉ**: ุฅุจุฑุงุฒ ุงููููุงุช ุงูููุฌูุฏุฉ ูู ูู ุฎุจุฑ
    - **ุชุญููู ุดุงูู**: ุฑุจุท ุงููููุงุช ุงูููุชุงุญูุฉ ุจุงููุชุงุฆุฌ
    - **ุงูุชุฑุงุญุงุช ุฐููุฉ**: ูุตุงุฆุญ ูุชุญุณูู ุงูุจุญุซ ุนูุฏ ุนุฏู ูุฌูุฏ ูุชุงุฆุฌ
    
    #### ๐ **ููุงุฆุฏ ุงููุธุงู ุงูุฌุฏูุฏ:**
    
    **ูููุณุชุฎุฏู ุงูุนุงุฏู:**
    - ุณูููุฉ ุงูุจุญุซ ุจุฃู ูููุฉ ูุฑูุฏูุง
    - ุนุฏู ุงูุญุงุฌุฉ ููุนุฑูุฉ ุงููููุงุช ุงููุญุฏุฏุฉ ูุณุจูุงู
    - ูุฑููุฉ ูู ุงูุชุนุจูุฑ ุนู ุงูุชูุงูุงุชู
    
    **ูููุณุชุฎุฏู ุงููุชูุฏู:**
    - ุจุญุซ ุฏููู ุจุนุจุงุฑุงุช ูุฎุตุตุฉ
    - ุงุณุชุฎุฏุงู ุงููุฑุงุฏูุงุช ููุจุญุซ ุงูุดุงูู
    - ุชุญููู ูุชูุฏู ููุชุงุฆุฌ ุงูุจุญุซ
    
    **ูููุทูุฑูู:**
    - ููุฏ ูุงุจู ููุชูุณูุน ูุงูุชุทููุฑ
    - ุฎูุงุฑุฒููุงุช ุจุญุซ ูุชูุฏูุฉ
    - ูุนุงูุฌุฉ ุฐููุฉ ูููุตูุต ุงูุนุฑุจูุฉ
    
    ### ๐ฏ **ุงูุฎูุงุตุฉ:**
    ุงููุธุงู ุงูุขู ูููุฑ **ุจุญุซ ููุชูุญ ูุญุฑ** ุจุฏูุงู ูู ุงูุงูุชุตุงุฑ ุนูู ูููุงุช ูุญุฏุฏุฉ ูุณุจูุงูุ ููุง ูุฌุนูู ุฃูุซุฑ ูุฑููุฉ ููุงุจููุฉ ููุงุณุชุฎุฏุงู ูู ุฌููุน ุงูุณููุงุฑูููุงุช ูุงูููุงุถูุน.
    """)
