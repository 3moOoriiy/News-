import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document

st.set_page_config(page_title="ðŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ðŸ—žï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© + Ù…ØµØ§Ø¯Ø± Ø£ÙƒØ«Ø±)")

# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ø³ÙŠØ§Ø³Ø©"],
    "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø¯ÙˆØ±ÙŠ", "Ù‡Ø¯Ù"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["Ø³ÙˆÙ‚", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "Ù…Ø§Ù„"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªØ·Ø¨ÙŠÙ‚", "Ù‡Ø§ØªÙ", "Ø°ÙƒØ§Ø¡", "Ø¨Ø±Ù…Ø¬Ø©"]
}

# Ø§Ù„Ø¯ÙˆØ§Ù„
def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

def analyze_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return "ðŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ðŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ðŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    for category, words in category_keywords.items():
        if any(word in text for word in words):
            return category
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

def fetch_news(source_name, url, keywords, date_from, date_to, chosen_category):
    feed = feedparser.parse(url)
    news_list = []
    for entry in feed.entries:
        title = entry.title
        summary = entry.get("summary", "")
        link = entry.link
        published = entry.get("published", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        try:
            published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
        except:
            published_dt = datetime.now()
        image = ""
        if 'media_content' in entry:
            image = entry.media_content[0].get('url', '')
        elif 'media_thumbnail' in entry:
            image = entry.media_thumbnail[0].get('url', '')

        if not (date_from <= published_dt.date() <= date_to):
            continue

        full_text = title + " " + summary
        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
            continue

        auto_category = detect_category(full_text)
        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
            continue

        news_list.append({
            "source": source_name,
            "title": title,
            "summary": summary,
            "link": link,
            "published": published_dt,
            "image": image,
            "sentiment": analyze_sentiment(summary),
            "category": auto_category
        })

    return news_list

def export_to_word(news_list):
    doc = Document()
    for news in news_list:
        doc.add_heading(news['title'], level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}  |  Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
        doc.add_paragraph(f"ðŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
        doc.add_paragraph("-----")
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (ØªÙ… Ø§Ù„ØªÙˆØ³ÙŠØ¹)
rss_feeds = {
    "BBC Ø¹Ø±Ø¨ÙŠ": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "CNN Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "http://arabic.cnn.com/rss/latest",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": "https://aawsat.com/home/rss.xml",
    "Ø³ÙƒØ§ÙŠ Ù†ÙŠÙˆØ² Ø¹Ø±Ø¨ÙŠØ©": "https://www.skynewsarabia.com/web/rss",
    "Ø§Ù„Ø¬Ø²ÙŠØ±Ø©": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "Ø¹Ø±Ø¨ÙŠ21": "https://arabi21.com/feed",
    "Ø§Ù„ÙˆØ·Ù†": "https://www.elwatannews.com/home/rss",
    "Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø³Ø§Ø¨Ø¹": "https://www.youm7.com/rss/SectionRss?SectionID=65",
    "Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„ÙŠÙˆÙ…": "https://www.almasryalyoum.com/rss/rssfeeds",
    "ØµØ­ÙŠÙØ© Ø³Ø¨Ù‚": "https://sabq.org/rss"
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­ÙƒÙ…
col1, col2 = st.columns([1, 2])
with col1:
    selected_source = st.selectbox("ðŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(rss_feeds.keys()))
    keywords_input = st.text_input("ðŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
    keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []
    category_filter = st.selectbox("ðŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()))
    date_from = st.date_input("ðŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
    date_to = st.date_input("ðŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())
    run = st.button("ðŸ“¥ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
with col2:
    if run:
        news = fetch_news(
            selected_source,
            rss_feeds[selected_source],
            keywords,
            date_from,
            date_to,
            category_filter
        )

        if not news:
            st.warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ·.")
        else:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø±.")
            for item in news:
                with st.container():
                    st.markdown("----")
                    cols = st.columns([1, 4])
                    with cols[0]:
                        if item["image"]:
                            st.image(item["image"], use_column_width=True)
                    with cols[1]:
                        st.markdown(f"### ðŸ“° {item['title']}")
                        st.markdown(f"ðŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {item['published'].strftime('%Y-%m-%d')}")
                        st.markdown(f"ðŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ: {item['category']}")
                        st.markdown(f"ðŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(item['summary'])}")
                        st.markdown(f"ðŸŽ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {item['sentiment']}")
                        st.markdown(f"[ðŸŒ Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ø²ÙŠØ¯ â†—]({item['link']})")

            word_file = export_to_word(news)
            excel_file = export_to_excel(news)

            st.download_button("ðŸ“„ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word", data=word_file, file_name="news.docx",
                               mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            st.download_button("ðŸ“Š ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel", data=excel_file, file_name="news.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            st.markdown("### ðŸ”  Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ù‹Ø§:")
            all_text = " ".join([n['summary'] for n in news])
            words = [word for word in all_text.split() if len(word) > 3]
            word_freq = Counter(words).most_common(10)
            for word, freq in word_freq:
                st.markdown(f"- **{word}**: {freq} Ù…Ø±Ø©")
