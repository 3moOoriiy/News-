import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document

st.set_page_config(page_title="ğŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ğŸ—ï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© + Ù…ØµØ§Ø¯Ø± Ø£ÙƒØ«Ø±)")

# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ø³ÙŠØ§Ø³Ø©", "Ø­ÙƒÙˆÙ…Ø©", "Ù†Ø§Ø¦Ø¨"],
    "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø¯ÙˆØ±ÙŠ", "Ù‡Ø¯Ù", "ÙØ±ÙŠÙ‚", "Ø¨Ø·ÙˆÙ„Ø©"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["Ø³ÙˆÙ‚", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "Ù…Ø§Ù„", "ØªØ¬Ø§Ø±Ø©", "ØµÙ†Ø§Ø¹Ø©"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªØ·Ø¨ÙŠÙ‚", "Ù‡Ø§ØªÙ", "Ø°ÙƒØ§Ø¡", "Ø¨Ø±Ù…Ø¬Ø©", "Ø¥Ù†ØªØ±Ù†Øª", "Ø±Ù‚Ù…ÙŠ"]
}

# Ø§Ù„Ø¯ÙˆØ§Ù„
def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

def analyze_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return "ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ğŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    for category, words in category_keywords.items():
        if any(word in text for word in words):
            return category
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

def fetch_rss_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± RSS"""
    try:
        feed = feedparser.parse(url)
        news_list = []
        for entry in feed.entries:
            title = entry.title
            summary = entry.get("summary", "")
            link = entry.link
            published = entry.get("published", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            try:
                published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
            except:
                try:
                    published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                except:
                    published_dt = datetime.now()
            
            image = ""
            if 'media_content' in entry:
                image = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry:
                image = entry.media_thumbnail[0].get('url', '')

            if not (date_from <= published_dt.date() <= date_to):
                continue

            full_text = title + " " + summary
            if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                continue

            auto_category = detect_category(full_text)
            if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
                continue

            news_list.append({
                "source": source_name,
                "title": title,
                "summary": summary,
                "link": link,
                "published": published_dt,
                "image": image,
                "sentiment": analyze_sentiment(summary),
                "category": auto_category
            })

        return news_list
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}: {str(e)}")
        return []

def try_multiple_rss_feeds(source_name, rss_options, keywords, date_from, date_to, chosen_category):
    """ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø· RSS Ù„Ù„Ù…ØµØ¯Ø± Ø§Ù„ÙˆØ§Ø­Ø¯"""
    for rss_url in rss_options:
        try:
            st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ¬Ø±Ø¨Ø©: {rss_url}")
            feed = feedparser.parse(rss_url)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ entries ÙÙŠ Ø§Ù„Ù€ feed
            if hasattr(feed, 'entries') and len(feed.entries) > 0:
                st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ RSS ÙÙŠ: {rss_url}")
                
                news_list = []
                for entry in feed.entries:
                    try:
                        title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
                        summary = entry.get('summary', entry.get('description', ''))
                        link = entry.get('link', '')
                        published = entry.get('published', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                        
                        try:
                            published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
                        except:
                            try:
                                published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                            except:
                                try:
                                    published_dt = datetime.strptime(published, "%Y-%m-%d %H:%M:%S")
                                except:
                                    published_dt = datetime.now()
                        
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®
                        if not (date_from <= published_dt.date() <= date_to):
                            continue

                        full_text = title + " " + summary
                        if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                            continue

                        auto_category = detect_category(full_text)
                        if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
                            continue

                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ±Ø©
                        image = ""
                        if hasattr(entry, 'media_content') and entry.media_content:
                            image = entry.media_content[0].get('url', '')
                        elif hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                            image = entry.media_thumbnail[0].get('url', '')
                        elif hasattr(entry, 'enclosures') and entry.enclosures:
                            for enclosure in entry.enclosures:
                                if 'image' in enclosure.get('type', ''):
                                    image = enclosure.get('href', '')
                                    break

                        news_list.append({
                            "source": source_name,
                            "title": title,
                            "summary": summary,
                            "link": link,
                            "published": published_dt,
                            "image": image,
                            "sentiment": analyze_sentiment(summary),
                            "category": auto_category,
                            "rss_url": rss_url
                        })
                    except Exception as e:
                        st.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø¨Ø±: {str(e)}")
                        continue
                
                if news_list:
                    return news_list
                else:
                    st.warning(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø´Ø±ÙˆØ· ÙÙŠ: {rss_url}")
                    
            else:
                st.warning(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ ÙÙŠ: {rss_url}")
                
        except Exception as e:
            st.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù€ {rss_url}: {str(e)}")
            continue
    
    return []

def export_to_word(news_list):
    doc = Document()
    doc.add_heading('ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 0)
    
    for news in news_list:
        doc.add_heading(news['title'], level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}  |  Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ğŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
        doc.add_paragraph(f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
        doc.add_paragraph("-----")
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©
general_rss_feeds = {
    "BBC Ø¹Ø±Ø¨ÙŠ": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "CNN Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "http://arabic.cnn.com/rss/latest",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": "https://aawsat.com/home/rss.xml",
    "Ø³ÙƒØ§ÙŠ Ù†ÙŠÙˆØ² Ø¹Ø±Ø¨ÙŠØ©": "https://www.skynewsarabia.com/web/rss",
    "Ø§Ù„Ø¬Ø²ÙŠØ±Ø©": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "Ø¹Ø±Ø¨ÙŠ21": "https://arabi21.com/feed",
    "Ø§Ù„ÙˆØ·Ù†": "https://www.elwatannews.com/home/rss",
    "Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø³Ø§Ø¨Ø¹": "https://www.youm7.com/rss/SectionRss?SectionID=65",
    "Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„ÙŠÙˆÙ…": "https://www.almasryalyoum.com/rss/rssfeeds",
    "ØµØ­ÙŠÙØ© Ø³Ø¨Ù‚": "https://sabq.org/rss"
}

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
iraqi_news_sources = {
    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://moi.gov.iq/",
        "type": "website",
        "rss_options": [
            "https://moi.gov.iq/feed/",
            "https://moi.gov.iq/rss.xml",
            "https://moi.gov.iq/wp-content/rss.xml"
        ]
    },
    "Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://hathalyoum.net/",
        "type": "website",
        "rss_options": [
            "https://hathalyoum.net/feed/",
            "https://hathalyoum.net/rss.xml",
            "https://hathalyoum.net/index.php/feed/"
        ]
    },
    "Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://iraqtoday.com/",
        "type": "website", 
        "rss_options": [
            "https://iraqtoday.com/feed/",
            "https://iraqtoday.com/rss.xml",
            "https://iraqtoday.com/wp-json/wp/v2/posts"
        ]
    },
    "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://presidency.iq/default.aspx",
        "type": "website",
        "rss_options": [
            "https://presidency.iq/feed/",
            "https://presidency.iq/rss.xml",
            "https://presidency.iq/ar/feed/"
        ]
    },
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": {
        "url": "https://asharq.com/",
        "type": "website",
        "rss_options": [
            "https://asharq.com/feed/",
            "https://asharq.com/rss.xml",
            "https://asharq.com/section/iraq/feed/"
        ]
    },
    "RT Arabic - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://arabic.rt.com/focuses/10744-%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss_options": [
            "https://arabic.rt.com/rss/",
            "https://arabic.rt.com/rss/focuses/10744/",
            "https://arabic.rt.com/feeds/iraq.rss"
        ]
    },
    "Ø¥Ù†Ø¯Ø¨Ù†Ø¯Ù†Øª Ø¹Ø±Ø¨ÙŠØ©": {
        "url": "https://www.independentarabia.com/",
        "type": "website",
        "rss_options": [
            "https://www.independentarabia.com/rss",
            "https://www.independentarabia.com/feed/",
            "https://www.independentarabia.com/tags/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/feed"
        ]
    },
    "ÙØ±Ø§Ù†Ø³ 24 Ø¹Ø±Ø¨ÙŠ": {
        "url": "https://www.france24.com/ar/",
        "type": "website",
        "rss_options": [
            "https://www.france24.com/ar/rss",
            "https://www.france24.com/ar/tag/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/rss",
            "https://www.france24.com/ar/programs/rss"
        ]
    }
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­ÙƒÙ…
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«")

# Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±
source_type = st.sidebar.selectbox(
    "ğŸŒ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±:",
    ["Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©", "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©"]
)

if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
    selected_source = st.sidebar.selectbox("ğŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(general_rss_feeds.keys()))
    source_url = general_rss_feeds[selected_source]
    source_info = {"type": "rss", "url": source_url}
else:
    selected_source = st.sidebar.selectbox("ğŸ‡®ğŸ‡¶ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ:", list(iraqi_news_sources.keys()))
    source_info = iraqi_news_sources[selected_source]

keywords_input = st.sidebar.text_input("ğŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []

category_filter = st.sidebar.selectbox("ğŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()))

date_from = st.sidebar.date_input("ğŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
date_to = st.sidebar.date_input("ğŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())

run = st.sidebar.button("ğŸ“¥ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", type="primary")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
col1, col2 = st.columns([2, 1])

with col1:
    if run:
        with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±..."):
            news = []
            
            if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
                news = fetch_rss_news(
                    selected_source,
                    source_info["url"],
                    keywords,
                    date_from,
                    date_to,
                    category_filter
                )
            else:  # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
                if source_info.get("rss_options"):
                    # ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø· RSS
                    news = try_multiple_rss_feeds(
                        selected_source,
                        source_info["rss_options"],
                        keywords,
                        date_from,
                        date_to,
                        category_filter
                    )
                
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙ†Ø¬Ø­ Ø£ÙŠ RSS
                if not news:
                    st.info(f"â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ RSS Ù…ØªØ§Ø­ Ù„Ù€ {selected_source}")
                    st.markdown(f"ğŸ”— **[Ø²ÙŠØ§Ø±Ø© {selected_source} Ù…Ø¨Ø§Ø´Ø±Ø©]({source_info['url']})**")
                    st.markdown("ğŸ’¡ ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

        if news:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø± Ù…Ù† {selected_source}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            for i, item in enumerate(news):
                with st.expander(f"ğŸ“° {item['title'][:100]}..."):
                    cols = st.columns([1, 3])
                    with cols[0]:
                        if item.get("image"):
                            try:
                                st.image(item["image"], use_column_width=True)
                            except:
                                st.write("ğŸ–¼ï¸ ØµÙˆØ±Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
                    
                    with cols[1]:
                        st.markdown(f"**ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®:** {item['published'].strftime('%Y-%m-%d %H:%M')}")
                        st.markdown(f"**ğŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ:** {item['category']}")
                        st.markdown(f"**ğŸ¢ Ø§Ù„Ù…ØµØ¯Ø±:** {item['source']}")
                        st.markdown(f"**ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:** {summarize(item['summary'], 50)}")
                        st.markdown(f"**ğŸ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ:** {item['sentiment']}")
                        st.markdown(f"**ğŸ”— [Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„Ø§Ù‹]({item['link']})**")

        elif run:
            st.warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ· Ø£Ùˆ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
with col2:
    if run and news:
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        categories = [n['category'] for n in news]
        category_counts = Counter(categories)
        
        st.write("**ğŸ“ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:**")
        for cat, count in category_counts.items():
            st.write(f"- {cat}: {count}")
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiments = [n['sentiment'] for n in news]
        sentiment_counts = Counter(sentiments)
        
        st.write("**ğŸ­ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:**")
        for sent, count in sentiment_counts.items():
            st.write(f"- {sent}: {count}")
        
        # Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹
        st.write("**ğŸ”¤ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹:**")
        all_text = " ".join([n['summary'] for n in news])
        words = [word for word in all_text.split() if len(word) > 3]
        word_freq = Counter(words).most_common(10)
        for word, freq in word_freq:
            st.write(f"- **{word}**: {freq}")

# Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
if run and news:
    st.subheader("ğŸ“¥ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    col_export1, col_export2 = st.columns(2)
    
    with col_export1:
        word_file = export_to_word(news)
        st.download_button(
            "ğŸ“„ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word",
            data=word_file,
            file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d')}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    
    with col_export2:
        excel_file = export_to_excel(news)
        st.download_button(
            "ğŸ“Š ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel",
            data=excel_file,
            file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
st.sidebar.markdown("---")
st.sidebar.info("""
ğŸ’¡ **Ù…Ù„Ø§Ø­Ø¸Ø§Øª:**
- ÙŠØªÙ… ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø±ÙˆØ§Ø¨Ø· RSS Ù„ÙƒÙ„ Ù…ØµØ¯Ø± Ø¹Ø±Ø§Ù‚ÙŠ
- Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ÙˆÙ‚Øª Ø£Ø·ÙˆÙ„ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
- Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¹Ù…Ù„ RSSØŒ ÙŠÙ…ÙƒÙ† Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©
- Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø­Ø¯Ø¯Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
""")

st.sidebar.markdown("### ğŸ‡®ğŸ‡¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:")
st.sidebar.markdown("""
- âœ… ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
- âœ… Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…  
- âœ… Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ…
- âœ… Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
- âœ… Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·
- âœ… RT Arabic - Ø§Ù„Ø¹Ø±Ø§Ù‚
- âœ… Ø¥Ù†Ø¯Ø¨Ù†Ø¯Ù†Øª Ø¹Ø±Ø¨ÙŠØ©
- âœ… ÙØ±Ø§Ù†Ø³ 24 Ø¹Ø±Ø¨ÙŠ
""")
