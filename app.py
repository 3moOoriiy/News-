import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
import urllib.request
import urllib.parse
import json
import re
import time

st.set_page_config(page_title="๐ฐ ุฃุฏุงุฉ ุงูุฃุฎุจุงุฑ ุงูุนุฑุจูุฉ ุงูุฐููุฉ", layout="wide")
st.title("๐๏ธ ุฃุฏุงุฉ ุฅุฏุงุฑุฉ ูุชุญููู ุงูุฃุฎุจุงุฑ ุงููุชุทูุฑุฉ (RSS + Web Scraping)")

# ุงูุชุตูููุงุช ุงููุญุณููุฉ
category_keywords = {
    "ุณูุงุณุฉ": ["ุฑุฆูุณ", "ูุฒูุฑ", "ุงูุชุฎุงุจุงุช", "ุจุฑููุงู", "ุณูุงุณุฉ", "ุญูููุฉ", "ูุงุฆุจ", "ูุฌูุณ", "ุฏููุฉ", "ุญุฒุจ"],
    "ุฑูุงุถุฉ": ["ูุฑุฉ", "ูุงุนุจ", "ูุจุงุฑุงุฉ", "ุฏูุฑู", "ูุฏู", "ูุฑูู", "ุจุทููุฉ", "ุฑูุงุถุฉ", "ููุนุจ", "ุชุฏุฑูุจ"],
    "ุงูุชุตุงุฏ": ["ุณูู", "ุงูุชุตุงุฏ", "ุงุณุชุซูุงุฑ", "ุจูู", "ูุงู", "ุชุฌุงุฑุฉ", "ุตูุงุนุฉ", "ููุท", "ุบุงุฒ", "ุจูุฑุตุฉ"],
    "ุชูููููุฌูุง": ["ุชูููุฉ", "ุชุทุจูู", "ูุงุชู", "ุฐูุงุก", "ุจุฑูุฌุฉ", "ุฅูุชุฑูุช", "ุฑููู", "ุญุงุณูุจ", "ุดุจูุฉ", "ุขูููู"],
    "ุตุญุฉ": ["ุทุจ", "ูุฑุถ", "ุนูุงุฌ", "ูุณุชุดูู", "ุฏูุงุก", "ุตุญุฉ", "ุทุจูุจ", "ููุฑูุณ", "ููุงุญ", "ูุจุงุก"],
    "ุชุนููู": ["ุชุนููู", "ุฌุงูุนุฉ", "ูุฏุฑุณุฉ", "ุทุงูุจ", "ุฏุฑุงุณุฉ", "ูููุฉ", "ูุนูุฏ", "ุชุฑุจูุฉ", "ุฃูุงุฏููู", "ุจุญุซ"]
}

# ุงูุฏูุงู ุงููุญุณููุฉ
def create_smart_summary(title, content, max_sentences=3):
    """ุฅูุดุงุก ููุฎุต ุฐูู ุฃุทูู"""
    if not content or content == title:
        # ุฅุฐุง ูู ููู ููุงู ูุญุชููุ ููุณุน ุงูุนููุงู
        words = title.split()
        if len(words) > 10:
            return " ".join(words[:15]) + "..."
        else:
            return title + " - ุชูุงุตูู ุฅุถุงููุฉ ูุชุงุญุฉ ูู ุงูููุงู ุงููุงูู."
    
    # ุชูุธูู ุงููุต
    content = re.sub(r'<[^>]+>', '', content)  # ุฅุฒุงูุฉ HTML tags
    content = re.sub(r'\s+', ' ', content).strip()  # ุชูุธูู ุงููุณุงูุงุช
    
    # ุชูุณูู ุฅูู ุฌูู
    sentences = re.split(r'[.!?]+', content)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    if not sentences:
        return title
    
    # ุฃุฎุฐ ุฃูู ุนุฏุฉ ุฌูู
    selected_sentences = sentences[:max_sentences]
    summary = ". ".join(selected_sentences)
    
    # ุงูุชุฃูุฏ ูู ุทูู ููุงุณุจ
    if len(summary.split()) < 20:
        # ุฅุถุงูุฉ ุงููุฒูุฏ ูู ุงูุฌูู ุฅุฐุง ูุงู ูุตูุฑ
        extra_sentences = sentences[max_sentences:max_sentences+2]
        if extra_sentences:
            summary += ". " + ". ".join(extra_sentences)
    
    # ูุทุน ุฅุฐุง ูุงู ุทููู ุฌุฏุงู
    words = summary.split()
    if len(words) > 80:
        summary = " ".join(words[:80]) + "..."
    
    return summary if summary else title

def analyze_sentiment(text):
    if not text:
        return "๐ ูุญุงูุฏ"
    try:
        polarity = TextBlob(text).sentiment.polarity
        if polarity > 0.1:
            return "๐ ุฅูุฌุงุจู"
        elif polarity < -0.1:
            return "๐ ุณูุจู"
        else:
            return "๐ ูุญุงูุฏ"
    except:
        return "๐ ูุญุงูุฏ"

def detect_category(text):
    if not text:
        return "ุบูุฑ ูุตููู"
    text_lower = text.lower()
    category_scores = {}
    
    for category, words in category_keywords.items():
        score = sum(1 for word in words if word in text_lower)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return "ุบูุฑ ูุตููู"

def safe_request(url, timeout=10):
    """ุทูุจ ุขูู ูุน ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        response = urllib.request.urlopen(req, timeout=timeout)
        return response.read().decode('utf-8', errors='ignore')
    except Exception as e:
        st.warning(f"ุฎุทุฃ ูู ุงููุตูู ูู {url}: {str(e)}")
        return None

def extract_news_from_html(html_content, source_name, base_url):
    """ุงุณุชุฎุฑุงุฌ ุงูุฃุฎุจุงุฑ ูู HTML ุจุทุฑููุฉ ุฐููุฉ"""
    if not html_content:
        return []
    
    news_list = []
    
    # ุงูุจุญุซ ุนู ุงูุนูุงููู ุงููุญุชููุฉ
    title_patterns = [
        r'<h[1-4][^>]*>(.*?)</h[1-4]>',
        r'<title[^>]*>(.*?)</title>',
        r'<a[^>]*title="([^"]+)"',
        r'<div[^>]*class="[^"]*title[^"]*"[^>]*>(.*?)</div>'
    ]
    
    # ุงูุจุญุซ ุนู ุงูุฑูุงุจุท
    link_patterns = [
        r'<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>',
        r'href="([^"]+)"'
    ]
    
    # ุงูุจุญุซ ุนู ุงููุตูุต ุงูุทูููุฉ (ููููุฎุตุงุช)
    content_patterns = [
        r'<p[^>]*>(.*?)</p>',
        r'<div[^>]*class="[^"]*content[^"]*"[^>]*>(.*?)</div>',
        r'<div[^>]*class="[^"]*summary[^"]*"[^>]*>(.*?)</div>'
    ]
    
    titles = []
    links = []
    contents = []
    
    for pattern in title_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        for match in matches:
            if isinstance(match, tuple):
                title = match[0] if match[0] else match[1] if len(match) > 1 else ""
            else:
                title = match
            title = re.sub(r'<[^>]+>', '', title).strip()
            if title and len(title) > 10 and len(title) < 200:
                titles.append(title)
    
    for pattern in link_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        for match in matches:
            if isinstance(match, tuple):
                link = match[0]
            else:
                link = match
            if link and not link.startswith('#') and not link.startswith('javascript:'):
                if link.startswith('/'):
                    link = base_url + link
                elif not link.startswith('http'):
                    link = base_url + '/' + link
                links.append(link)
    
    # ุงุณุชุฎุฑุงุฌ ุงููุตูุต ููููุฎุตุงุช
    for pattern in content_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        for match in matches:
            content = re.sub(r'<[^>]+>', '', str(match)).strip()
            if content and len(content) > 50:
                contents.append(content)
    
    # ุฏูุฌ ุงูุนูุงููู ูุงูุฑูุงุจุท ูุงููุญุชููุงุช
    for i, title in enumerate(titles[:10]):  # ุฃูู 10 ุฃุฎุจุงุฑ
        link = links[i] if i < len(links) else base_url
        content = contents[i] if i < len(contents) else title
        
        # ุฅูุดุงุก ููุฎุต ูุญุณู
        smart_summary = create_smart_summary(title, content)
        
        news_list.append({
            "source": source_name,
            "title": title,
            "summary": smart_summary,
            "link": link,
            "published": datetime.now(),
            "image": "",
            "sentiment": analyze_sentiment(smart_summary),
            "category": detect_category(title + " " + smart_summary),
            "extraction_method": "HTML Parsing"
        })
    
    return news_list

def fetch_rss_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """ุฌูุจ ุงูุฃุฎุจุงุฑ ูู RSS ูุน ููุฎุตุงุช ูุญุณูุฉ"""
    try:
        feed = feedparser.parse(url)
        news_list = []
        
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        for entry in feed.entries:
            try:
                title = entry.get('title', 'ุจุฏูู ุนููุงู')
                summary = entry.get('summary', entry.get('description', ''))
                content = entry.get('content', [{}])
                if content and isinstance(content, list) and len(content) > 0:
                    full_content = content[0].get('value', summary)
                else:
                    full_content = summary
                
                link = entry.get('link', '')
                published = entry.get('published', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                
                # ูุนุงูุฌุฉ ุงูุชุงุฑูุฎ
                try:
                    published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
                except:
                    try:
                        published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                    except:
                        published_dt = datetime.now()
                
                # ููุชุฑุฉ ุงูุชุงุฑูุฎ
                if not (date_from <= published_dt.date() <= date_to):
                    continue

                # ุฅูุดุงุก ููุฎุต ุฐูู ูุญุณู
                enhanced_summary = create_smart_summary(title, full_content)
                
                # ููุชุฑุฉ ุงููููุงุช ุงูููุชุงุญูุฉ
                full_text = title + " " + enhanced_summary
                if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                    continue

                # ููุชุฑุฉ ุงูุชุตููู
                auto_category = detect_category(full_text)
                if chosen_category != "ุงููู" and auto_category != chosen_category:
                    continue

                # ุงูุจุญุซ ุนู ุตูุฑุฉ
                image = ""
                if hasattr(entry, 'media_content') and entry.media_content:
                    image = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image = entry.media_thumbnail[0].get('url', '')

                news_list.append({
                    "source": source_name,
                    "title": title,
                    "summary": enhanced_summary,
                    "link": link,
                    "published": published_dt,
                    "image": image,
                    "sentiment": analyze_sentiment(enhanced_summary),
                    "category": auto_category,
                    "extraction_method": "RSS"
                })
                
            except Exception as e:
                continue
                
        return news_list
        
    except Exception as e:
        return []

def fetch_website_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """ุฌูุจ ุงูุฃุฎุจุงุฑ ูู ุงููููุน ูุจุงุดุฑุฉ"""
    try:
        st.info(f"๐ ุฌุงุฑู ุชุญููู ูููุน {source_name}...")
        
        # ุฌูุจ ูุญุชูู ุงูุตูุญุฉ
        html_content = safe_request(url)
        if not html_content:
            return []
        
        # ุงุณุชุฎุฑุงุฌ ุงูุฃุฎุจุงุฑ ูู HTML
        base_url = url.rstrip('/')
        news_list = extract_news_from_html(html_content, source_name, base_url)
        
        # ููุชุฑุฉ ุงููุชุงุฆุฌ
        filtered_news = []
        for news in news_list:
            # ููุชุฑุฉ ุงููููุงุช ุงูููุชุงุญูุฉ
            full_text = news['title'] + " " + news['summary']
            if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                continue
            
            # ููุชุฑุฉ ุงูุชุตููู
            if chosen_category != "ุงููู" and news['category'] != chosen_category:
                continue
            
            filtered_news.append(news)
        
        return filtered_news[:10]  # ุฃูู 10 ุฃุฎุจุงุฑ
        
    except Exception as e:
        st.error(f"ุฎุทุฃ ูู ุฌูุจ ุงูุฃุฎุจุงุฑ ูู {source_name}: {str(e)}")
        return []

def smart_news_fetcher(source_name, source_info, keywords, date_from, date_to, chosen_category):
    """ุฌุงูุจ ุงูุฃุฎุจุงุฑ ุงูุฐูู - ูุฌุฑุจ ุนุฏุฉ ุทุฑู"""
    all_news = []
    
    # ุงููุญุงููุฉ ุงูุฃููู: RSS
    if source_info.get("rss_options"):
        st.info("๐ ุงููุญุงููุฉ ุงูุฃููู: ุงูุจุญุซ ุนู RSS...")
        for rss_url in source_info["rss_options"]:
            try:
                news = fetch_rss_news(source_name, rss_url, keywords, date_from, date_to, chosen_category)
                if news:
                    st.success(f"โ ุชู ุงูุนุซูุฑ ุนูู {len(news)} ุฎุจุฑ ูู RSS: {rss_url}")
                    all_news.extend(news)
                    break
            except:
                continue
    
    # ุงููุญุงููุฉ ุงูุซุงููุฉ: ุชุญููู ุงููููุน ูุจุงุดุฑุฉ
    if not all_news:
        st.info("๐ ุงููุญุงููุฉ ุงูุซุงููุฉ: ุชุญููู ุงููููุน ูุจุงุดุฑุฉ...")
        website_news = fetch_website_news(source_name, source_info["url"], keywords, date_from, date_to, chosen_category)
        if website_news:
            st.success(f"โ ุชู ุงุณุชุฎุฑุงุฌ {len(website_news)} ุฎุจุฑ ูู ุงููููุน ูุจุงุดุฑุฉ")
            all_news.extend(website_news)
    
    # ุฅุฒุงูุฉ ุงูููุฑุฑ
    seen_titles = set()
    unique_news = []
    for news in all_news:
        if news['title'] not in seen_titles:
            seen_titles.add(news['title'])
            unique_news.append(news)
    
    return unique_news

def export_to_word(news_list):
    doc = Document()
    doc.add_heading('ุชูุฑูุฑ ุงูุฃุฎุจุงุฑ ุงููุฌูุนุฉ', 0)
    doc.add_paragraph(f'ุชุงุฑูุฎ ุงูุชูุฑูุฑ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    doc.add_paragraph(f'ุนุฏุฏ ุงูุฃุฎุจุงุฑ: {len(news_list)}')
    doc.add_paragraph('---')
    
    for i, news in enumerate(news_list, 1):
        doc.add_heading(f'{i}. {news["title"]}', level=2)
        doc.add_paragraph(f"ุงููุตุฏุฑ: {news['source']}")
        doc.add_paragraph(f"ุงูุชุตููู: {news['category']}")
        doc.add_paragraph(f"ุงูุชุงุฑูุฎ: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ุทุฑููุฉ ุงูุงุณุชุฎุฑุงุฌ: {news.get('extraction_method', 'ุบูุฑ ูุญุฏุฏ')}")
        doc.add_paragraph(f"ุงูุชุญููู ุงูุนุงุทูู: {news['sentiment']}")
        doc.add_paragraph(f"ุงูููุฎุต: {news['summary']}")
        doc.add_paragraph(f"ุงูุฑุงุจุท: {news['link']}")
        doc.add_paragraph('---')
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    # ุชุฑุชูุจ ุงูุฃุนูุฏุฉ
    columns_order = ['source', 'title', 'category', 'sentiment', 'published', 'summary', 'link', 'extraction_method']
    df = df.reindex(columns=[col for col in columns_order if col in df.columns])
    
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ุงูุฃุฎุจุงุฑ')
    buffer.seek(0)
    return buffer

# ูุตุงุฏุฑ ุงูุฃุฎุจุงุฑ ุงููุญุณููุฉ
general_rss_feeds = {
    "BBC ุนุฑุจู": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "ุงูุฌุฒูุฑุฉ": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 ุนุฑุจู": "https://www.france24.com/ar/rss",
    "ุณูุงู ูููุฒ ุนุฑุจูุฉ": "https://www.skynewsarabia.com/web/rss",
    "ุนุฑุจู21": "https://arabi21.com/feed"
}

iraqi_news_sources = {
    "ูุฒุงุฑุฉ ุงูุฏุงุฎููุฉ ุงูุนุฑุงููุฉ": {
        "url": "https://moi.gov.iq/",
        "type": "website",
        "rss_options": [
            "https://moi.gov.iq/feed/",
            "https://moi.gov.iq/rss.xml"
        ]
    },
    "ูุฐุง ุงูููู": {
        "url": "https://hathalyoum.net/",
        "type": "website",
        "rss_options": [
            "https://hathalyoum.net/feed/",
            "https://hathalyoum.net/rss.xml"
        ]
    },
    "ุงูุนุฑุงู ุงูููู": {
        "url": "https://iraqtoday.com/",
        "type": "website",
        "rss_options": [
            "https://iraqtoday.com/feed/",
            "https://iraqtoday.com/rss.xml"
        ]
    },
    "ุฑุฆุงุณุฉ ุงูุฌูููุฑูุฉ ุงูุนุฑุงููุฉ": {
        "url": "https://presidency.iq/default.aspx",
        "type": "website",
        "rss_options": [
            "https://presidency.iq/feed/",
            "https://presidency.iq/rss.xml"
        ]
    },
    "ุงูุดุฑู ุงูุฃูุณุท": {
        "url": "https://asharq.com/",
        "type": "website",
        "rss_options": [
            "https://asharq.com/feed/",
            "https://asharq.com/rss.xml"
        ]
    },
    "RT Arabic - ุงูุนุฑุงู": {
        "url": "https://arabic.rt.com/focuses/10744-%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss_options": [
            "https://arabic.rt.com/rss/"
        ]
    },
    "ุฅูุฏุจูุฏูุช ุนุฑุจูุฉ": {
        "url": "https://www.independentarabia.com/",
        "type": "website",
        "rss_options": [
            "https://www.independentarabia.com/rss"
        ]
    },
    "ูุฑุงูุณ 24 ุนุฑุจู": {
        "url": "https://www.france24.com/ar/",
        "type": "website",
        "rss_options": [
            "https://www.france24.com/ar/rss"
        ]
    }
}

# ูุงุฌูุฉ ุงููุณุชุฎุฏู ุงููุญุณููุฉ
st.sidebar.header("โ๏ธ ุฅุนุฏุงุฏุงุช ุงูุจุญุซ ุงููุชูุฏู")

# ุงุฎุชูุงุฑ ููุน ุงููุตุฏุฑ
source_type = st.sidebar.selectbox(
    "๐ ุงุฎุชุฑ ููุน ุงููุตุฏุฑ:",
    ["ุงููุตุงุฏุฑ ุงูุนุงูุฉ", "ุงููุตุงุฏุฑ ุงูุนุฑุงููุฉ"],
    help="ุงููุตุงุฏุฑ ุงูุนุงูุฉ ุชุนุชูุฏ ุนูู RSSุ ุงููุตุงุฏุฑ ุงูุนุฑุงููุฉ ุชุณุชุฎุฏู ุชูููุงุช ูุชูุฏูุฉ"
)

if source_type == "ุงููุตุงุฏุฑ ุงูุนุงูุฉ":
    selected_source = st.sidebar.selectbox("๐ ุงุฎุชุฑ ูุตุฏุฑ ุงูุฃุฎุจุงุฑ:", list(general_rss_feeds.keys()))
    source_url = general_rss_feeds[selected_source]
    source_info = {"type": "rss", "url": source_url}
else:
    selected_source = st.sidebar.selectbox("๐ฎ๐ถ ุงุฎุชุฑ ูุตุฏุฑ ุงูุฃุฎุจุงุฑ ุงูุนุฑุงูู:", list(iraqi_news_sources.keys()))
    source_info = iraqi_news_sources[selected_source]

# ุฅุนุฏุงุฏุงุช ุงูุจุญุซ
keywords_input = st.sidebar.text_input(
    "๐ ูููุงุช ููุชุงุญูุฉ (ููุตููุฉ ุจููุงุตู):", 
    "",
    help="ูุซุงู: ุณูุงุณุฉุ ุงูุชุตุงุฏุ ุจุบุฏุงุฏ"
)
keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []

category_filter = st.sidebar.selectbox(
    "๐ ุงุฎุชุฑ ุงูุชุตููู:", 
    ["ุงููู"] + list(category_keywords.keys()),
    help="ููุชุฑุฉ ุงูุฃุฎุจุงุฑ ุญุณุจ ุงูุชุตููู"
)

# ุฅุนุฏุงุฏุงุช ุงูุชุงุฑูุฎ
col_date1, col_date2 = st.sidebar.columns(2)
with col_date1:
    date_from = st.date_input("๐ ูู ุชุงุฑูุฎ:", datetime.today() - timedelta(days=7))
with col_date2:
    date_to = st.date_input("๐ ุฅูู ุชุงุฑูุฎ:", datetime.today())

# ุฎูุงุฑุงุช ูุชูุฏูุฉ
with st.sidebar.expander("โ๏ธ ุฎูุงุฑุงุช ูุชูุฏูุฉ"):
    max_news = st.slider("ุนุฏุฏ ุงูุฃุฎุจุงุฑ ุงูุฃูุตู:", 5, 50, 20)
    include_sentiment = st.checkbox("ุชุญููู ุงููุดุงุนุฑ", True)
    include_categorization = st.checkbox("ุงูุชุตููู ุงูุชููุงุฆู", True)

run = st.sidebar.button("๐ฅ ุฌูุจ ุงูุฃุฎุจุงุฑ", type="primary", help="ุงุจุฏุฃ ุนูููุฉ ุฌูุจ ูุชุญููู ุงูุฃุฎุจุงุฑ")

# ุนุฑุถ ุงููุชุงุฆุฌ
if run:
    with st.spinner("๐ค ุฌุงุฑู ุชุดุบูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุฌูุจ ุงูุฃุฎุจุงุฑ..."):
        start_time = time.time()
        
        if source_type == "ุงููุตุงุฏุฑ ุงูุนุงูุฉ":
            news = fetch_rss_news(
                selected_source,
                source_info["url"],
                keywords,
                date_from,
                date_to,
                category_filter
            )
        else:
            news = smart_news_fetcher(
                selected_source,
                source_info,
                keywords,
                date_from,
                date_to,
                category_filter
            )
        
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
    
    if news:
        st.success(f"๐ ุชู ุฌูุจ {len(news)} ุฎุจุฑ ูู {selected_source} ูู {processing_time} ุซุงููุฉ")
        
        # ุฅุญุตุงุฆูุงุช ุณุฑูุนุฉ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("๐ฐ ุฅุฌูุงูู ุงูุฃุฎุจุงุฑ", len(news))
        with col2:
            categories = [n['category'] for n in news]
            st.metric("๐ ุฃูุซุฑ ุชุตููู", Counter(categories).most_common(1)[0][0] if categories else "ุบูุฑ ูุญุฏุฏ")
        with col3:
            positive_news = len([n for n in news if "ุฅูุฌุงุจู" in n['sentiment']])
            st.metric("๐ ุฃุฎุจุงุฑ ุฅูุฌุงุจูุฉ", positive_news)
        with col4:
            st.metric("โฑ๏ธ ููุช ุงููุนุงูุฌุฉ", f"{processing_time}s")
        
        # ุนุฑุถ ุงูุฃุฎุจุงุฑ ุจุชุตููู ูุญุณู
        st.subheader("๐ ุงูุฃุฎุจุงุฑ ุงููุฌูุนุฉ")
        
        for i, item in enumerate(news[:max_news], 1):
            # ุญุงููุฉ ุฑุฆูุณูุฉ ูุน ุชุตููู ุฃููู
            with st.container():
                # ุฅูุดุงุก ุฃุนูุฏุฉ ููุชุฎุทูุท
                if item.get('image'):
                    col_image, col_content = st.columns([1, 4])  # ุนููุฏ ุตุบูุฑ ููุตูุฑุฉุ ูุจูุฑ ูููุญุชูู
                    
                    with col_image:
                        st.image(
                            item['image'], 
                            width=120,  # ุชุตุบูุฑ ุญุฌู ุงูุตูุฑุฉ
                            caption="",
                            use_column_width=False
                        )
                    
                    with col_content:
                        # ุงูุนููุงู
                        st.markdown(f"### ๐ฐ {item['title']}")
                        
                        # ูุนูููุงุช ุณุฑูุนุฉ ูู ุตู ูุงุญุฏ
                        info_col1, info_col2, info_col3, info_col4 = st.columns(4)
                        with info_col1:
                            st.markdown(f"**๐ข {item['source']}**")
                        with info_col2:
                            st.markdown(f"**๐ {item['category']}**")
                        with info_col3:
                            st.markdown(f"**๐ญ {item['sentiment']}**")
                        with info_col4:
                            st.markdown(f"**๐ {item['published'].strftime('%m-%d %H:%M')}**")
                        
                        # ุงูููุฎุต ุงููุญุณู
                        st.markdown("**๐ ุงูููุฎุต ุงูุชูุตููู:**")
                        st.markdown(f">{item['summary']}")
                        
                        # ุงูุฑุงุจุท
                        st.markdown(f"๐ **[ูุฑุงุกุฉ ุงูููุงู ูุงููุงู โ]({item['link']})**")
                
                else:
                    # ุชุฎุทูุท ุจุฏูู ุตูุฑุฉ
                    st.markdown(f"### ๐ฐ {item['title']}")
                    
                    # ูุนูููุงุช ุณุฑูุนุฉ
                    info_col1, info_col2, info_col3, info_col4, info_col5 = st.columns(5)
                    with info_col1:
                        st.markdown(f"**๐ข {item['source']}**")
                    with info_col2:
                        st.markdown(f"**๐ {item['category']}**")
                    with info_col3:
                        st.markdown(f"**๐ญ {item['sentiment']}**")
                    with info_col4:
                        st.markdown(f"**๐ {item['published'].strftime('%Y-%m-%d')}**")
                    with info_col5:
                        st.markdown(f"**๐ง {item.get('extraction_method', 'ุบูุฑ ูุญุฏุฏ')}**")
                    
                    # ุงูููุฎุต ูู ูุฑุจุน ูููุตู
                    st.markdown("**๐ ุงูููุฎุต ุงูุชูุตููู:**")
                    st.info(item['summary'])
                    
                    # ุงูุฑุงุจุท
                    st.markdown(f"๐ **[ูุฑุงุกุฉ ุงูููุงู ูุงููุงู โ]({item['link']})**")
                
                # ุฎุท ูุงุตู ุฃููู
                st.markdown("---")
        
        # ุชุตุฏูุฑ ุงูุจูุงูุงุช
        st.subheader("๐ค ุชุตุฏูุฑ ุงูุจูุงูุงุช")
        col_export1, col_export2, col_export3 = st.columns(3)
        
        with col_export1:
            word_file = export_to_word(news)
            st.download_button(
                "๐ ุชุญููู Word",
                data=word_file,
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        
        with col_export2:
            excel_file = export_to_excel(news)
            st.download_button(
                "๐ ุชุญููู Excel",
                data=excel_file,
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        with col_export3:
            json_data = json.dumps(news, ensure_ascii=False, default=str, indent=2)
            st.download_button(
                "๐พ ุชุญููู JSON",
                data=json_data.encode('utf-8'),
                file_name=f"ุงุฎุจุงุฑ_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                mime="application/json"
            )
        
        # ุชุญูููุงุช ูุชูุฏูุฉ
        with st.expander("๐ ุชุญูููุงุช ูุชูุฏูุฉ"):
            col_analysis1, col_analysis2 = st.columns(2)
            
            with col_analysis1:
                st.subheader("๐ ุชูุฒูุน ุงูุชุตูููุงุช")
                categories = [n['category'] for n in news]
                category_counts = Counter(categories)
                
                for cat, count in category_counts.most_common():
                    percentage = (count / len(news)) * 100
                    st.write(f"โข **{cat}**: {count} ({percentage:.1f}%)")
            
            with col_analysis2:
                st.subheader("๐ญ ุชุญููู ุงููุดุงุนุฑ")
                sentiments = [n['sentiment'] for n in news]
                sentiment_counts = Counter(sentiments)
                
                for sent, count in sentiment_counts.items():
                    percentage = (count / len(news)) * 100
                    st.write(f"โข **{sent}**: {count} ({percentage:.1f}%)")
            
            st.subheader("๐ค ุฃูุซุฑ ุงููููุงุช ุชูุฑุงุฑุงู")
            all_text = " ".join([n['title'] + " " + n['summary'] for n in news])
            # ุชูุธูู ุงููุต
            words = re.findall(r'\b[ุฃ-ู]{3,}\b', all_text)  # ูููุงุช ุนุฑุจูุฉ ููุท
            word_freq = Counter(words).most_common(15)
            
            if word_freq:
                cols = st.columns(3)
                for i, (word, freq) in enumerate(word_freq):
                    with cols[i % 3]:
                        st.write(f"**{word}**: {freq} ูุฑุฉ")
    
    else:
        st.warning("โ ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃุฎุจุงุฑ ุจุงูุดุฑูุท ุงููุญุฏุฏุฉ")
        st.info("๐ก ุฌุฑุจ ุชูุณูุน ูุทุงู ุงูุชุงุฑูุฎ ุฃู ุชุบููุฑ ุงููููุงุช ุงูููุชุงุญูุฉ")
        st.markdown(f"๐ **[ุฒูุงุฑุฉ {selected_source} ูุจุงุดุฑุฉ]({source_info['url']})**")

# ูุนูููุงุช ูู ุงูุดุฑูุท ุงูุฌุงูุจู
st.sidebar.markdown("---")
st.sidebar.info("""
๐ **ุชูููุงุช ูุชูุฏูุฉ:**
- ุฌูุจ RSS ุชููุงุฆู
- ุชุญููู ููุงูุน ุงูููุจ
- ุชุตููู ุฐูู ููุฃุฎุจุงุฑ
- ุชุญููู ุงููุดุงุนุฑ
- ุฅุฒุงูุฉ ุงููุญุชูู ุงูููุฑุฑ
- ููุฎุตุงุช ุฐููุฉ ูุทููุฉ
""")

st.sidebar.success("โ ูุธุงู ุฐูู ูุชุทูุฑ ูุฌูุน ุงูุฃุฎุจุงุฑ!")

# ูุนูููุงุช ุชูููุฉ
with st.expander("โน๏ธ ูุนูููุงุช ุชูููุฉ - ุงูุชุญุณููุงุช ุงูุฌุฏูุฏุฉ"):
    st.markdown("""
    ### ๐๏ธ ุงูุชุญุณููุงุช ุงููุถุงูุฉ:
    
    #### ๐ **ุชุญุณูู ุงูููุฎุตุงุช:**
    - **ููุฎุตุงุช ุฃุทูู ูุฃูุซุฑ ุชูุตููุงู**: ูุชู ุฅูุดุงุก ููุฎุตุงุช ุชุชุฑุงูุญ ุจูู 20-80 ูููุฉ
    - **ุงุณุชุฎุฑุงุฌ ุฐูู ูููุญุชูู**: ุงุณุชุฎุฏุงู ุนุฏุฉ ุฃููุงุท ูุงุณุชุฎุฑุงุฌ ุงููุตูุต ุงููุงููุฉ
    - **ุฏูุฌ ุงูุนูุงููู ูุงููุญุชูู**: ุฑุจุท ุงูุนูุงููู ุจุงููุตูุต ูุฅูุดุงุก ููุฎุตุงุช ุดุงููุฉ
    - **ุชูุธูู ูุชูุฏู ูููุตูุต**: ุฅุฒุงูุฉ HTML tags ูุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    
    #### ๐ผ๏ธ **ุชุญุณูู ุนุฑุถ ุงูุตูุฑ:**
    - **ุญุฌู ูุตุบุฑ ููุตูุฑ**: ุนุฑุถ 120 ุจูุณู ุจุฏูุงู ูู ุงูุนุฑุถ ุงููุงูู
    - **ุชุฎุทูุท ูุญุณู**: ุงุณุชุฎุฏุงู ุฃุนูุฏุฉ ูููุตูุฉ ููุตูุฑ ูุงููุญุชูู
    - **ุนุฑุถ ุดุฑุทู**: ุฅุธูุงุฑ ุงูุตูุฑ ููุท ุนูุฏ ุชููุฑูุง
    - **ุชุญุณูู ุงูุงุณุชุฌุงุจุฉ**: ุชุฎุทูุท ูุชููู ูุน ูุฌูุฏ ุฃู ุนุฏู ูุฌูุฏ ุตูุฑ
    
    #### ๐จ **ุชุญุณูู ุชุตููู ุงูุฃุฎุจุงุฑ:**
    - **ุชุฎุทูุท ุฃููู ุจุงูุฃุนูุฏุฉ**: ูุนูููุงุช ููุธูุฉ ูู ุตููู ูุฃุนูุฏุฉ
    - **ูุนูููุงุช ุณุฑูุนุฉ**: ุนุฑุถ ุงููุตุฏุฑ ูุงูุชุตููู ูุงูุชุงุฑูุฎ ูู ุตู ูุงุญุฏ
    - **ูุฑุจุนุงุช ูุนูููุงุช**: ุงุณุชุฎุฏุงู `st.info()` ูุนุฑุถ ุงูููุฎุตุงุช ุจุดูู ุจุงุฑุฒ
    - **ููุงุตู ุฃูููุฉ**: ุฎุทูุท ูุงุตูุฉ ุจูู ุงูุฃุฎุจุงุฑ ูุณูููุฉ ุงููุฑุงุกุฉ
    
    #### ๐ง **ุชุญุณููุงุช ุชูููุฉ:**
    - **ุงุณุชุฎุฑุงุฌ ูุญุชูู ูุชูุฏู**: ุฃููุงุท regex ูุญุณูุฉ ูุงุณุชุฎุฑุงุฌ ุงููุตูุต
    - **ูุนุงูุฌุฉ ุฃุฎุทุงุก ูุญุณูุฉ**: ุงูุชุนุงูู ูุน ุงููุญุชูู ุงูููููุฏ ุฃู ุงูุชุงูู
    - **ุฐุงูุฑุฉ ูุญุณูุฉ**: ุชุฌูุจ ุชูุฑุงุฑ ุงููุนุงูุฌุฉ ูููุตูุต ุงูุทูููุฉ
    - **ุฃุฏุงุก ุฃูุถู**: ุชุญุณูู ุณุฑุนุฉ ูุนุงูุฌุฉ ุงูููุฎุตุงุช ุงูุทูููุฉ
    
    ### ๐ **ูุฒุงูุง ุงููุธุงู ุงููุญุณู:**
    - **ููุฎุตุงุช ุฃูุซุฑ ูุงุฆุฏุฉ**: ูุนูููุงุช ุดุงููุฉ ุจุฏูุงู ูู ุงูุนูุงููู ููุท
    - **ุนุฑุถ ุฃุฌูู**: ูุงุฌูุฉ ููุธูุฉ ูุณููุฉ ุงููุฑุงุกุฉ
    - **ุตูุฑ ููุงุณุจุฉ ุงูุญุฌู**: ูุง ุชุดุบู ูุณุงุญุฉ ูุจูุฑุฉ ูู ุงูุดุงุดุฉ
    - **ุชุฌุฑุจุฉ ูุณุชุฎุฏู ูุญุณูุฉ**: ุชููู ุฃุณูู ููุฑุงุกุฉ ุฃูุถุญ
    - **ูุนูููุงุช ุณุฑูุนุฉ**: ูู ูุง ุชุญุชุงุฌู ูู ููุงู ูุงุญุฏ
    
    ### ๐ฏ **ููููุฉ ุนูู ุงูููุฎุตุงุช ุงูุฐููุฉ:**
    1. **ุงุณุชุฎุฑุงุฌ ุงููุญุชูู ุงููุงูู**: ูู RSS ุฃู HTML
    2. **ุชูุธูู ุงููุตูุต**: ุฅุฒุงูุฉ ุงูุนูุงูุงุช ูุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    3. **ุชูุณูู ุฅูู ุฌูู**: ูุตู ุงููุต ุฅูู ุฌูู ููุทููุฉ
    4. **ุงุฎุชูุงุฑ ุฃูุถู ุงูุฌูู**: ุฃูู 2-3 ุฌูู ุฃู ุญุชู 80 ูููุฉ
    5. **ุงูุชุญูู ูู ุงูุทูู**: ุถูุงู ููุฎุต ููุงุณุจ ุงูุทูู
    6. **ุฅุถุงูุฉ ูุญุชูู ุฅุถุงูู**: ุนูุฏ ุงูุญุงุฌุฉ ูููุฎุต ุฃุทูู
    """)
