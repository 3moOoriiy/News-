import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
import urllib.request
import urllib.parse
import json
import re
import time

st.set_page_config(page_title="ğŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ğŸ—ï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© (RSS + Web Scraping)")

# Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ØªØ¬Ù†Ø¨Ù‡Ø§
STOP_WORDS = {
    'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ù…Ø¹', 'Ø¨Ø¹Ø¯', 'Ù‚Ø¨Ù„', 'ØªØ­Øª', 'ÙÙˆÙ‚', 'Ø­ÙˆÙ„', 'Ø®Ù„Ø§Ù„', 'Ø¹Ø¨Ø±', 'Ø¶Ø¯', 'Ù†Ø­Ùˆ', 'Ø¹Ù†Ø¯', 'Ù„Ø¯Ù‰',
    'Ø£Ù†', 'Ø¥Ù†', 'ÙƒØ§Ù†', 'ÙƒØ§Ù†Øª', 'ÙŠÙƒÙˆÙ†', 'ØªÙƒÙˆÙ†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'Ù‡Ù…', 'Ù‡Ù†', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ†', 'Ø£Ù†Ø§', 'Ù†Ø­Ù†',
    'Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ø°Ù„Ùƒ', 'ØªÙ„Ùƒ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¨Ø¹Ø¶', 'ÙƒÙ„', 'Ø¬Ù…ÙŠØ¹',
    'Ø£Ùˆ', 'Ø£Ù…', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†', 'ØºÙŠØ±', 'Ø³ÙˆÙ‰', 'ÙÙ‚Ø·', 'Ø£ÙŠØ¶Ø§', 'Ø£ÙŠØ¶Ø§Ù‹', 'ÙƒØ°Ù„Ùƒ', 'Ø£ÙŠØ¶Ø§Ù‹', 'Ø­ÙŠØ«', 'Ø¨ÙŠÙ†Ù…Ø§', 'ÙƒÙ…Ø§',
    'Ù‚Ø¯', 'Ù„Ù‚Ø¯', 'Ù‚Ø§Ù„', 'Ù‚Ø§Ù„Øª', 'Ø£Ø¶Ø§Ù', 'Ø£Ø¶Ø§ÙØª', 'Ø£ÙƒØ¯', 'Ø£ÙƒØ¯Øª', 'Ø°ÙƒØ±', 'Ø°ÙƒØ±Øª', 'Ø£Ø´Ø§Ø±', 'Ø£Ø´Ø§Ø±Øª'
}

def clean_text_for_analysis(text):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©"""
    if not text:
        return ""
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† HTML ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª ÙÙ‚Ø·
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ÙˆÙÙ„ØªØ±Ø©
    words = text.split()
    filtered_words = []
    
    for word in words:
        word = word.strip()
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© (Ø£Ù‚Ù„ Ù…Ù† 3 Ø£Ø­Ø±Ù) ÙˆØ­Ø±ÙˆÙ Ø§Ù„Ø¬Ø±
        if len(word) >= 3 and word not in STOP_WORDS:
            filtered_words.append(word)
    
    return ' '.join(filtered_words)

def extract_meaningful_words(text, min_length=3, max_words=50):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø© ÙÙ‚Ø·"""
    if not text:
        return []
    
    cleaned_text = clean_text_for_analysis(text)
    words = cleaned_text.split()
    
    # ÙÙ„ØªØ±Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©
    meaningful_words = []
    for word in words:
        if (len(word) >= min_length and 
            word not in STOP_WORDS and 
            not word.isdigit() and
            len(word) <= 15):  # ØªØ¬Ù†Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
            meaningful_words.append(word)
    
    return meaningful_words[:max_words]

def open_search(text, search_terms):
    """Ø¨Ø­Ø« Ù…ÙØªÙˆØ­ ØªÙ…Ø§Ù…Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø©"""
    if not text or not search_terms:
        return True  # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ ÙƒÙ„Ù…Ø§Øª Ø¨Ø­Ø«ØŒ Ø§Ø¹Ø±Ø¶ ÙƒÙ„ Ø´ÙŠØ¡
    
    text_clean = text.lower().strip()
    
    for term in search_terms:
        term = term.strip().lower()
        if not term or len(term) < 2:
            continue
            
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        if term in text_clean:
            return True
            
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
        if len(term) > 5:
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙ„Ù…Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø²Ø¦ÙŠ
            for i in range(len(term) - 3):
                part = term[i:i+4]
                if part in text_clean:
                    return True
    
    return False

def smart_categorize(text):
    """ØªØµÙ†ÙŠÙ Ø°ÙƒÙŠ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆÙ„ÙŠØ³ ÙƒÙ„Ù…Ø§Øª Ù…Ø³Ø¬Ù„Ø©"""
    if not text:
        return "ØºÙŠØ± Ù…ØµÙ†Ù"
    
    text_lower = text.lower()
    
    # ÙƒÙ„Ù…Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙØ§Øª (ÙŠÙ…ÙƒÙ† ØªÙˆØ³ÙŠØ¹Ù‡Ø§ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹)
    category_patterns = {
        "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø­ÙƒÙˆÙ…Ø©", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ù…Ø¬Ù„Ø³", "Ø¯ÙˆÙ„Ø©", "Ø³ÙŠØ§Ø³Ø©", "Ù‚Ø§Ù†ÙˆÙ†", "Ø¹Ø¯Ø§Ù„Ø©"],
        "Ø§Ù‚ØªØµØ§Ø¯": ["Ø§Ù‚ØªØµØ§Ø¯", "Ù…Ø§Ù„", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "ØªØ¬Ø§Ø±Ø©", "Ø³ÙˆÙ‚", "Ø£Ø³Ù‡Ù…", "Ø¹Ù…Ù„Ø©", "Ù†ÙØ·", "Ø·Ø§Ù‚Ø©"],
        "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "ÙØ±ÙŠÙ‚", "Ø¨Ø·ÙˆÙ„Ø©", "Ø¯ÙˆØ±ÙŠ", "Ø±ÙŠØ§Ø¶Ø©", "Ù…Ù„Ø¹Ø¨", "ØªØ¯Ø±ÙŠØ¨", "Ù†Ø§Ø¯ÙŠ"],
        "ØµØ­Ø©": ["ØµØ­Ø©", "Ø·Ø¨", "Ù…Ø±Ø¶", "Ø¹Ù„Ø§Ø¬", "Ù…Ø³ØªØ´ÙÙ‰", "Ø¯ÙˆØ§Ø¡", "ÙÙŠØ±ÙˆØ³", "Ù„Ù‚Ø§Ø­", "Ø·Ø¨ÙŠØ¨", "Ù…Ø±ÙŠØ¶"],
        "ØªØ¹Ù„ÙŠÙ…": ["ØªØ¹Ù„ÙŠÙ…", "Ø¬Ø§Ù…Ø¹Ø©", "Ù…Ø¯Ø±Ø³Ø©", "Ø·Ø§Ù„Ø¨", "Ù…Ø¹Ù„Ù…", "Ø¯Ø±Ø§Ø³Ø©", "ØªØ±Ø¨ÙŠØ©", "Ø§Ù…ØªØ­Ø§Ù†", "ÙƒÙ„ÙŠØ©", "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"],
        "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", "ÙƒÙ…Ø¨ÙŠÙˆØªØ±", "Ø¥Ù†ØªØ±Ù†Øª", "ØªØ·Ø¨ÙŠÙ‚", "Ø¨Ø±Ù…Ø¬Ø©", "Ø°ÙƒØ§Ø¡", "Ø±Ù‚Ù…ÙŠ", "Ù‡Ø§ØªÙ", "Ø´Ø¨ÙƒØ©"]
    }
    
    scores = {}
    for category, words in category_patterns.items():
        score = sum(1 for word in words if word in text_lower)
        if score > 0:
            scores[category] = score
    
    return max(scores, key=scores.get) if scores else "Ø¹Ø§Ù…"

def analyze_sentiment_simple(text):
    """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ø´Ø§Ø¹Ø±"""
    if not text:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"
    
    positive_words = ["Ù†Ø¬Ø­", "ØªÙ‚Ø¯Ù…", "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ø¬ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "ØªØ·ÙˆØ±", "Ø§Ø²Ø¯Ù‡Ø§Ø±", "Ø§Ù†ØªØµØ§Ø±", "ÙÙˆØ²"]
    negative_words = ["ÙØ´Ù„", "Ø³ÙŠØ¡", "Ø®Ø·Ø£", "Ù…Ø´ÙƒÙ„Ø©", "Ø£Ø²Ù…Ø©", "ØªØ±Ø§Ø¬Ø¹", "Ø§Ù†Ù‡ÙŠØ§Ø±", "Ù‡Ø²ÙŠÙ…Ø©", "ÙƒØ§Ø±Ø«Ø©", "Ù‚Ù„Ù‚"]
    
    text_lower = text.lower()
    
    positive_count = sum(1 for word in positive_words if word in text_lower)
    negative_count = sum(1 for word in negative_words if word in text_lower)
    
    if positive_count > negative_count:
        return "ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif negative_count > positive_count:
        return "ğŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def create_enhanced_summary(title, content, max_length=200):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù…Ø­Ø³Ù† ÙˆØ·ÙˆÙŠÙ„"""
    if not content or content.strip() == title.strip():
        return title + " - Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ØŒ ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ."
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    content_clean = re.sub(r'<[^>]+>', '', content)
    content_clean = re.sub(r'\s+', ' ', content_clean).strip()
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø°ÙƒØ§Ø¡
    if title not in content_clean:
        full_text = title + ". " + content_clean
    else:
        full_text = content_clean
    
    # ØªÙ‚Ø·ÙŠØ¹ Ø¥Ù„Ù‰ Ø¬Ù…Ù„
    sentences = re.split(r'[.!?]+', full_text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    if not sentences:
        return title
    
    # Ø¨Ù†Ø§Ø¡ Ù…Ù„Ø®Øµ ØªØ¯Ø±ÙŠØ¬ÙŠ
    summary = ""
    for sentence in sentences:
        if len(summary + sentence) <= max_length:
            summary += sentence + ". "
        else:
            break
    
    if not summary.strip():
        summary = title
    
    # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù‚Ø·ÙˆØ¹Ø§Ù‹
    if len(full_text) > len(summary) and not summary.endswith("..."):
        summary = summary.rstrip(". ") + "..."
    
    return summary.strip()

def safe_web_request(url, timeout=10):
    """Ø·Ù„Ø¨ ÙˆÙŠØ¨ Ø¢Ù…Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ø£Ø®Ø·Ø§Ø¡"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=timeout) as response:
            return response.read().decode('utf-8', errors='ignore')
    except Exception as e:
        st.warning(f"ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ {url}: {str(e)}")
        return None

def fetch_rss_news(source_name, url, search_terms, date_from, date_to):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS Ù…Ø¹ Ø¨Ø­Ø« Ù…ÙØªÙˆØ­"""
    try:
        with st.spinner(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}..."):
            feed = feedparser.parse(url)
            
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        news_list = []
        
        for entry in feed.entries:
            try:
                title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†').strip()
                summary = entry.get('summary', entry.get('description', '')).strip()
                
                # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù† ÙˆØ¬Ø¯
                content = entry.get('content', [])
                if content and isinstance(content, list) and len(content) > 0:
                    full_content = content[0].get('value', summary)
                else:
                    full_content = summary
                
                link = entry.get('link', '')
                published_str = entry.get('published', '')
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®
                try:
                    if published_str:
                        # ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù„Ù„ØªØ§Ø±ÙŠØ®
                        for fmt in ['%a, %d %b %Y %H:%M:%S %Z', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d %H:%M:%S']:
                            try:
                                published_dt = datetime.strptime(published_str.strip(), fmt)
                                break
                            except:
                                continue
                        else:
                            published_dt = datetime.now()
                    else:
                        published_dt = datetime.now()
                except:
                    published_dt = datetime.now()
                
                # ÙÙ„ØªØ±Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®
                if not (date_from <= published_dt.date() <= date_to):
                    continue
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù…Ø­Ø³Ù†
                enhanced_summary = create_enhanced_summary(title, full_content)
                
                # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙØªÙˆØ­
                full_search_text = title + " " + enhanced_summary
                if search_terms and not open_search(full_search_text, search_terms):
                    continue
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ±Ø©
                image_url = ""
                if hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url', '')
                
                news_item = {
                    "source": source_name,
                    "title": title,
                    "summary": enhanced_summary,
                    "link": link,
                    "published": published_dt,
                    "image": image_url,
                    "sentiment": analyze_sentiment_simple(enhanced_summary),
                    "category": smart_categorize(full_search_text)
                }
                
                news_list.append(news_item)
                
            except Exception as e:
                continue
        
        return news_list[:50]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50 Ø®Ø¨Ø±
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}: {str(e)}")
        return []

def export_to_excel(news_data):
    """ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Excel"""
    df = pd.DataFrame(news_data)
    buffer = BytesIO()
    
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Ø§Ù„Ø£Ø®Ø¨Ø§Ø±')
    
    buffer.seek(0)
    return buffer

def export_to_word(news_data):
    """ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Word"""
    doc = Document()
    doc.add_heading('ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 0)
    doc.add_paragraph(f'ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime("%Y-%m-%d %H:%M")}')
    doc.add_paragraph(f'Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {len(news_data)}')
    doc.add_paragraph('---')
    
    for i, news in enumerate(news_data, 1):
        doc.add_heading(f'{i}. {news["title"]}', level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}")
        doc.add_paragraph(f"Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {news['sentiment']}")
        doc.add_paragraph(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M')}")
        doc.add_paragraph(f"Ø§Ù„Ù…Ù„Ø®Øµ: {news['summary']}")
        doc.add_paragraph(f"Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph('---')
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
NEWS_SOURCES = {
    "Ø§Ù„Ø¬Ø²ÙŠØ±Ø©": "https://www.aljazeera.net/aljazeerarss/a7c186be-1baa-4bd4-9d80-a84db769f779/73d0e1b4-532f-45ef-b135-bfdff8b8cab9",
    "BBC Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "https://www.alarabiya.net/ar/rss.xml",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø³ÙƒØ§ÙŠ Ù†ÙŠÙˆØ² Ø¹Ø±Ø¨ÙŠØ©": "https://www.skynewsarabia.com/web/rss",
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": "https://aawsat.com/rss/latest",
    "CNN Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "https://arabic.cnn.com/api/v1/rss/rss.xml"
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.sidebar.header("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙØªÙˆØ­ ÙˆØ§Ù„Ù…Ø±Ù†")

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØµØ¯Ø±
selected_source = st.sidebar.selectbox(
    "ğŸ“¡ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:",
    list(NEWS_SOURCES.keys()),
    help="Ø§Ø®ØªØ± Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡"
)

# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙØªÙˆØ­
st.sidebar.markdown("### ğŸ†“ Ø¨Ø­Ø« Ø­Ø± ÙˆÙ…ÙØªÙˆØ­")
search_input = st.sidebar.text_area(
    "Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡ ØªØ±ÙŠØ¯Ù‡:",
    value="",
    height=120,
    help="""
    ğŸ” Ø§Ø¨Ø­Ø« Ø¨Ø­Ø±ÙŠØ© ØªØ§Ù…Ø© Ø¹Ù† Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹:
    
    Ù…Ø«Ø§Ù„:
    â€¢ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø©: ÙƒÙˆØ±ÙˆÙ†Ø§
    â€¢ Ø¹Ø¯Ø© ÙƒÙ„Ù…Ø§Øª: Ø¨ØºØ¯Ø§Ø¯ØŒ Ø§Ù„Ø±Ø¦ÙŠØ³ØŒ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù…  
    â€¢ Ø¹Ø¨Ø§Ø±Ø©: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    â€¢ Ù…ÙˆØ¶ÙˆØ¹: Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„Ø¹Ø±Ø§Ù‚
    
    âœ¨ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù‚ÙŠÙˆØ¯ - Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø´ÙŠØ¡!
    """
)

# Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
search_terms = []
if search_input.strip():
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¨Ø§Ù„ÙÙˆØ§ØµÙ„ Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    raw_terms = re.split(r'[ØŒ,\s]+', search_input.strip())
    search_terms = [term.strip() for term in raw_terms if term.strip() and len(term.strip()) > 1]

# Ø¹Ø±Ø¶ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
if search_terms:
    st.sidebar.success(f"âœ… Ø³ÙŠØªÙ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {len(search_terms)} ÙƒÙ„Ù…Ø©/Ø¹Ø¨Ø§Ø±Ø©")
    with st.sidebar.expander("ğŸ“ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©"):
        for i, term in enumerate(search_terms, 1):
            st.write(f"{i}. **{term}**")
else:
    st.sidebar.info("ğŸŒ Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®
col1, col2 = st.sidebar.columns(2)
with col1:
    date_from = st.date_input("Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today() - timedelta(days=7))
with col2:
    date_to = st.date_input("Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
with st.sidebar.expander("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"):
    max_articles = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø£Ù‚ØµÙ‰:", 5, 50, 20)
    show_images = st.checkbox("Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±", True)
    detailed_analysis = st.checkbox("ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„", True)

# Ø²Ø± Ø§Ù„Ø¨Ø­Ø«
search_button = st.sidebar.button("ğŸ” Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ø­Ø«", type="primary")

# Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
if search_button:
    if date_from > date_to:
        st.error("âŒ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø¨Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©")
        st.stop()
    
    # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
    st.info(f"""
    ğŸ¯ **ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«:**
    - Ø§Ù„Ù…ØµØ¯Ø±: **{selected_source}**
    - ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«: **{len(search_terms)}** {"ÙƒÙ„Ù…Ø©" if search_terms else "Ø¨Ø­Ø« Ø´Ø§Ù…Ù„"}
    - Ø§Ù„ÙØªØ±Ø©: Ù…Ù† **{date_from}** Ø¥Ù„Ù‰ **{date_to}**
    """)
    
    if search_terms:
        st.success(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {', '.join(search_terms[:5])}{'...' if len(search_terms) > 5 else ''}")
    
    # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    start_time = time.time()
    
    news_data = fetch_rss_news(
        selected_source,
        NEWS_SOURCES[selected_source],
        search_terms,
        date_from,
        date_to
    )
    
    processing_time = round(time.time() - start_time, 2)
    
    if news_data:
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        news_data = news_data[:max_articles]
        
        st.success(f"ğŸ‰ ØªÙ… Ø¬Ù„Ø¨ {len(news_data)} Ø®Ø¨Ø± ÙÙŠ {processing_time} Ø«Ø§Ù†ÙŠØ©")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø©
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ“° Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", len(news_data))
        
        with col2:
            categories = [item['category'] for item in news_data]
            top_category = Counter(categories).most_common(1)[0][0] if categories else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            st.metric("ğŸ“ Ø£ÙƒØ«Ø± ØªØµÙ†ÙŠÙ", top_category)
        
        with col3:
            sentiments = [item['sentiment'] for item in news_data]
            positive_count = len([s for s in sentiments if "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" in s])
            st.metric("ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", positive_count)
        
        with col4:
            st.metric("âš¡ Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", f"{processing_time}s")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        if detailed_analysis:
            st.subheader("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰")
            
            all_text = " ".join([item['title'] + " " + item['summary'] for item in news_data])
            meaningful_words = extract_meaningful_words(all_text, min_length=3, max_words=100)
            
            if meaningful_words:
                word_counts = Counter(meaningful_words).most_common(20)
                
                st.markdown("**ğŸ”¤ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:**")
                
                cols = st.columns(4)
                for i, (word, count) in enumerate(word_counts):
                    with cols[i % 4]:
                        percentage = (count / len(news_data)) * 100
                        st.metric(word, f"{count}", f"{percentage:.1f}%")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        st.subheader("ğŸ“° Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«")
        
        for i, article in enumerate(news_data, 1):
            with st.container():
                # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                st.markdown(f"### {i}. {article['title']}")
                
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ø±ÙŠØ¹Ø©
                info_cols = st.columns(4)
                with info_cols[0]:
                    st.markdown(f"**ğŸ“¡ {article['source']}**")
                with info_cols[1]:
                    st.markdown(f"**ğŸ“ {article['category']}**")
                with info_cols[2]:
                    st.markdown(f"**ğŸ­ {article['sentiment']}**")
                with info_cols[3]:
                    st.markdown(f"**ğŸ“… {article['published'].strftime('%m-%d %H:%M')}**")
                
                # Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ù…Ù„Ø®Øµ
                if show_images and article.get('image'):
                    col_img, col_text = st.columns([1, 4])
                    with col_img:
                        st.image(article['image'], width=100)
                    with col_text:
                        st.markdown("**ğŸ“ Ø§Ù„Ù…Ù„Ø®Øµ:**")
                        st.write(article['summary'])
                else:
                    st.markdown("**ğŸ“ Ø§Ù„Ù…Ù„Ø®Øµ:**")
                    st.info(article['summary'])
                
                # Ø§Ù„Ø±Ø§Ø¨Ø·
                st.markdown(f"ğŸ”— **[Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„Ø§Ù‹]({article['link']})**")
                
                # Ø¥Ø¨Ø±Ø§Ø² Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
                if search_terms:
                    article_text = (article['title'] + " " + article['summary']).lower()
                    matched_terms = [term for term in search_terms if term.lower() in article_text]
                    if matched_terms:
                        st.success(f"ğŸ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø©: {', '.join(matched_terms)}")
                
                st.markdown("---")
        
        # Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØµØ¯ÙŠØ±
        st.subheader("ğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            excel_file = export_to_excel(news_data)
            st.download_button(
                "ğŸ“Š ØªØ­Ù…ÙŠÙ„ Excel",
                data=excel_file,
                file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        with col2:
            word_file = export_to_word(news_data)
            st.download_button(
                "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Word",
                data=word_file,
                file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        
        with col3:
            json_data = json.dumps(news_data, ensure_ascii=False, default=str, indent=2)
            st.download_button(
                "ğŸ’¾ ØªØ­Ù…ÙŠÙ„ JSON",
                data=json_data.encode('utf-8'),
                file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                mime="application/json"
            )
    
    else:
        st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø¨Ø­Ø«")
        
        st.markdown("### ğŸ’¡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª:")
        st.markdown("""
        - Ø¬Ø±Ø¨ ÙƒÙ„Ù…Ø§Øª Ø¨Ø­Ø« Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø£ÙƒØ«Ø± Ø¹Ù…ÙˆÙ…ÙŠØ©
        - ÙˆØ³Ø¹ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ù„Ø¨Ø­Ø«  
        - ØªØ£ÙƒØ¯ Ù…Ù† ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        - Ø¬Ø±Ø¨ Ù…ØµØ¯Ø± Ø£Ø®Ø¨Ø§Ø± Ø¢Ø®Ø±
        """)

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.markdown("---")
st.sidebar.success("âœ… **Ø¨Ø­Ø« Ù…ÙØªÙˆØ­ 100%**")
st.sidebar.info("""
ğŸ” **Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø¯ÙŠØ¯:**
- Ø¨Ø­Ø« Ø­Ø± Ø¨Ø£ÙŠ ÙƒÙ„Ù…Ø© ØªØ±ÙŠØ¯Ù‡Ø§
- Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
- ØªÙ†Ø¸ÙŠÙ Ø°ÙƒÙŠ Ù„Ù„Ù†ØµÙˆØµ
- Ø¥Ø²Ø§Ù„Ø© Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
- ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ù…ØªÙ‚Ø¯Ù…
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø© ÙÙ‚Ø·
""")

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©
with st.expander("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©"):
    st.markdown("""
    ### ğŸ”§ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©:
    
    **Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙØªÙˆØ­:**
    - Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    - Ø¨Ø­Ø« Ù…Ø±Ù† ÙÙŠ Ø£ÙŠ Ù†Øµ Ø£Ùˆ ÙƒÙ„Ù…Ø©
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    
    **ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ:**
    - Ø¥Ø²Ø§Ù„Ø© Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø± ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©
    - ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯Ø©
    - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© ÙˆØ§Ù„Ù…ÙÙŠØ¯Ø© ÙÙ‚Ø·
    
    **Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø°ÙƒÙŠ:**
    - ØªØµÙ†ÙŠÙ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    - Ø¹Ø¯Ù… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¦Ù… ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
    - ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ§Ù‚ÙŠ Ù„Ù„Ù†ØµÙˆØµ
    """)
