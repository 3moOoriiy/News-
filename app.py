import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
import urllib.request
import urllib.parse
import json
import re
import time

st.set_page_config(page_title="ğŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ğŸ—ï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØªØ·ÙˆØ±Ø© (RSS + Web Scraping)")

# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ø³ÙŠØ§Ø³Ø©", "Ø­ÙƒÙˆÙ…Ø©", "Ù†Ø§Ø¦Ø¨", "Ù…Ø¬Ù„Ø³", "Ø¯ÙˆÙ„Ø©", "Ø­Ø²Ø¨"],
    "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø¯ÙˆØ±ÙŠ", "Ù‡Ø¯Ù", "ÙØ±ÙŠÙ‚", "Ø¨Ø·ÙˆÙ„Ø©", "Ø±ÙŠØ§Ø¶Ø©", "Ù…Ù„Ø¹Ø¨", "ØªØ¯Ø±ÙŠØ¨"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["Ø³ÙˆÙ‚", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "Ù…Ø§Ù„", "ØªØ¬Ø§Ø±Ø©", "ØµÙ†Ø§Ø¹Ø©", "Ù†ÙØ·", "ØºØ§Ø²", "Ø¨ÙˆØ±ØµØ©"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªØ·Ø¨ÙŠÙ‚", "Ù‡Ø§ØªÙ", "Ø°ÙƒØ§Ø¡", "Ø¨Ø±Ù…Ø¬Ø©", "Ø¥Ù†ØªØ±Ù†Øª", "Ø±Ù‚Ù…ÙŠ", "Ø­Ø§Ø³ÙˆØ¨", "Ø´Ø¨ÙƒØ©", "Ø¢ÙŠÙÙˆÙ†"],
    "ØµØ­Ø©": ["Ø·Ø¨", "Ù…Ø±Ø¶", "Ø¹Ù„Ø§Ø¬", "Ù…Ø³ØªØ´ÙÙ‰", "Ø¯ÙˆØ§Ø¡", "ØµØ­Ø©", "Ø·Ø¨ÙŠØ¨", "ÙÙŠØ±ÙˆØ³", "Ù„Ù‚Ø§Ø­", "ÙˆØ¨Ø§Ø¡"],
    "ØªØ¹Ù„ÙŠÙ…": ["ØªØ¹Ù„ÙŠÙ…", "Ø¬Ø§Ù…Ø¹Ø©", "Ù…Ø¯Ø±Ø³Ø©", "Ø·Ø§Ù„Ø¨", "Ø¯Ø±Ø§Ø³Ø©", "ÙƒÙ„ÙŠØ©", "Ù…Ø¹Ù‡Ø¯", "ØªØ±Ø¨ÙŠØ©", "Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ", "Ø¨Ø­Ø«"]
}

# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
def summarize(text, max_words=30):
    if not text:
        return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ Ù…ØªØ§Ø­"
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + "..."

def analyze_sentiment(text):
    if not text:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"
    try:
        polarity = TextBlob(text).sentiment.polarity
        if polarity > 0.1:
            return "ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
        elif polarity < -0.1:
            return "ğŸ˜  Ø³Ù„Ø¨ÙŠ"
        else:
            return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"
    except:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    if not text:
        return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"
    text_lower = text.lower()
    category_scores = {}
    
    for category, words in category_keywords.items():
        score = sum(1 for word in words if word in text_lower)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

def safe_request(url, timeout=10):
    """Ø·Ù„Ø¨ Ø¢Ù…Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        response = urllib.request.urlopen(req, timeout=timeout)
        return response.read().decode('utf-8', errors='ignore')
    except Exception as e:
        st.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù€ {url}: {str(e)}")
        return None

def extract_news_from_html(html_content, source_name, base_url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† HTML Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø°ÙƒÙŠØ©"""
    if not html_content:
        return []
    
    news_list = []
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
    title_patterns = [
        r'<h[1-4][^>]*>(.*?)</h[1-4]>',
        r'<title[^>]*>(.*?)</title>',
        r'<a[^>]*title="([^"]+)"',
        r'<div[^>]*class="[^"]*title[^"]*"[^>]*>(.*?)</div>'
    ]
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    link_patterns = [
        r'<a[^>]*href="([^"]+)"[^>]*>(.*?)</a>',
        r'href="([^"]+)"'
    ]
    
    titles = []
    links = []
    
    for pattern in title_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
        for match in matches:
            if isinstance(match, tuple):
                title = match[0] if match[0] else match[1] if len(match) > 1 else ""
            else:
                title = match
            title = re.sub(r'<[^>]+>', '', title).strip()
            if title and len(title) > 10 and len(title) < 200:
                titles.append(title)
    
    for pattern in link_patterns:
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        for match in matches:
            if isinstance(match, tuple):
                link = match[0]
            else:
                link = match
            if link and not link.startswith('#') and not link.startswith('javascript:'):
                if link.startswith('/'):
                    link = base_url + link
                elif not link.startswith('http'):
                    link = base_url + '/' + link
                links.append(link)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·
    for i, title in enumerate(titles[:10]):  # Ø£ÙˆÙ„ 10 Ø£Ø®Ø¨Ø§Ø±
        link = links[i] if i < len(links) else base_url
        
        news_list.append({
            "source": source_name,
            "title": title,
            "summary": title,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙƒÙ…Ù„Ø®Øµ Ù…Ø¤Ù‚Øª
            "link": link,
            "published": datetime.now(),
            "image": "",
            "sentiment": analyze_sentiment(title),
            "category": detect_category(title),
            "extraction_method": "HTML Parsing"
        })
    
    return news_list

def fetch_rss_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† RSS"""
    try:
        feed = feedparser.parse(url)
        news_list = []
        
        if not hasattr(feed, 'entries') or len(feed.entries) == 0:
            return []
        
        for entry in feed.entries:
            try:
                title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
                summary = entry.get('summary', entry.get('description', title))
                link = entry.get('link', '')
                published = entry.get('published', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®
                try:
                    published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
                except:
                    try:
                        published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                    except:
                        published_dt = datetime.now()
                
                # ÙÙ„ØªØ±Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®
                if not (date_from <= published_dt.date() <= date_to):
                    continue

                # ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                full_text = title + " " + summary
                if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                    continue

                # ÙÙ„ØªØ±Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
                auto_category = detect_category(full_text)
                if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
                    continue

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ±Ø©
                image = ""
                if hasattr(entry, 'media_content') and entry.media_content:
                    image = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image = entry.media_thumbnail[0].get('url', '')

                news_list.append({
                    "source": source_name,
                    "title": title,
                    "summary": summary,
                    "link": link,
                    "published": published_dt,
                    "image": image,
                    "sentiment": analyze_sentiment(summary),
                    "category": auto_category,
                    "extraction_method": "RSS"
                })
                
            except Exception as e:
                continue
                
        return news_list
        
    except Exception as e:
        return []

def fetch_website_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©"""
    try:
        st.info(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ‚Ø¹ {source_name}...")
        
        # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
        html_content = safe_request(url)
        if not html_content:
            return []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† HTML
        base_url = url.rstrip('/')
        news_list = extract_news_from_html(html_content, source_name, base_url)
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        filtered_news = []
        for news in news_list:
            # ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            full_text = news['title'] + " " + news['summary']
            if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                continue
            
            # ÙÙ„ØªØ±Ø© Ø§Ù„ØªØµÙ†ÙŠÙ
            if chosen_category != "Ø§Ù„ÙƒÙ„" and news['category'] != chosen_category:
                continue
            
            filtered_news.append(news)
        
        return filtered_news[:10]  # Ø£ÙˆÙ„ 10 Ø£Ø®Ø¨Ø§Ø±
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}: {str(e)}")
        return []

def smart_news_fetcher(source_name, source_info, keywords, date_from, date_to, chosen_category):
    """Ø¬Ø§Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ - ÙŠØ¬Ø±Ø¨ Ø¹Ø¯Ø© Ø·Ø±Ù‚"""
    all_news = []
    
    # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: RSS
    if source_info.get("rss_options"):
        st.info("ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† RSS...")
        for rss_url in source_info["rss_options"]:
            try:
                news = fetch_rss_news(source_name, rss_url, keywords, date_from, date_to, chosen_category)
                if news:
                    st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø± Ù…Ù† RSS: {rss_url}")
                    all_news.extend(news)
                    break
            except:
                continue
    
    # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©
    if not all_news:
        st.info("ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©...")
        website_news = fetch_website_news(source_name, source_info["url"], keywords, date_from, date_to, chosen_category)
        if website_news:
            st.success(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(website_news)} Ø®Ø¨Ø± Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø©")
            all_news.extend(website_news)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±
    seen_titles = set()
    unique_news = []
    for news in all_news:
        if news['title'] not in seen_titles:
            seen_titles.add(news['title'])
            unique_news.append(news)
    
    return unique_news

def export_to_word(news_list):
    doc = Document()
    doc.add_heading('ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©', 0)
    doc.add_paragraph(f'ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    doc.add_paragraph(f'Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±: {len(news_list)}')
    doc.add_paragraph('---')
    
    for i, news in enumerate(news_list, 1):
        doc.add_heading(f'{i}. {news["title"]}', level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}")
        doc.add_paragraph(f"Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {news.get('extraction_method', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
        doc.add_paragraph(f"Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ: {news['sentiment']}")
        doc.add_paragraph(f"Ø§Ù„Ù…Ù„Ø®Øµ: {news['summary']}")
        doc.add_paragraph(f"Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph('---')
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    columns_order = ['source', 'title', 'category', 'sentiment', 'published', 'summary', 'link', 'extraction_method']
    df = df.reindex(columns=[col for col in columns_order if col in df.columns])
    
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Ø§Ù„Ø£Ø®Ø¨Ø§Ø±')
    buffer.seek(0)
    return buffer

# Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
general_rss_feeds = {
    "BBC Ø¹Ø±Ø¨ÙŠ": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "Ø§Ù„Ø¬Ø²ÙŠØ±Ø©": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø³ÙƒØ§ÙŠ Ù†ÙŠÙˆØ² Ø¹Ø±Ø¨ÙŠØ©": "https://www.skynewsarabia.com/web/rss",
    "Ø¹Ø±Ø¨ÙŠ21": "https://arabi21.com/feed"
}

iraqi_news_sources = {
    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://moi.gov.iq/",
        "type": "website",
        "rss_options": [
            "https://moi.gov.iq/feed/",
            "https://moi.gov.iq/rss.xml"
        ]
    },
    "Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://hathalyoum.net/",
        "type": "website",
        "rss_options": [
            "https://hathalyoum.net/feed/",
            "https://hathalyoum.net/rss.xml"
        ]
    },
    "Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://iraqtoday.com/",
        "type": "website",
        "rss_options": [
            "https://iraqtoday.com/feed/",
            "https://iraqtoday.com/rss.xml"
        ]
    },
    "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://presidency.iq/default.aspx",
        "type": "website",
        "rss_options": [
            "https://presidency.iq/feed/",
            "https://presidency.iq/rss.xml"
        ]
    },
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": {
        "url": "https://asharq.com/",
        "type": "website",
        "rss_options": [
            "https://asharq.com/feed/",
            "https://asharq.com/rss.xml"
        ]
    },
    "RT Arabic - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://arabic.rt.com/focuses/10744-%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss_options": [
            "https://arabic.rt.com/rss/"
        ]
    },
    "Ø¥Ù†Ø¯Ø¨Ù†Ø¯Ù†Øª Ø¹Ø±Ø¨ÙŠØ©": {
        "url": "https://www.independentarabia.com/",
        "type": "website",
        "rss_options": [
            "https://www.independentarabia.com/rss"
        ]
    },
    "ÙØ±Ø§Ù†Ø³ 24 Ø¹Ø±Ø¨ÙŠ": {
        "url": "https://www.france24.com/ar/",
        "type": "website",
        "rss_options": [
            "https://www.france24.com/ar/rss"
        ]
    }
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")

# Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±
source_type = st.sidebar.selectbox(
    "ğŸŒ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±:",
    ["Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©", "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©"],
    help="Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ RSSØŒ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"
)

if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
    selected_source = st.sidebar.selectbox("ğŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(general_rss_feeds.keys()))
    source_url = general_rss_feeds[selected_source]
    source_info = {"type": "rss", "url": source_url}
else:
    selected_source = st.sidebar.selectbox("ğŸ‡®ğŸ‡¶ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ:", list(iraqi_news_sources.keys()))
    source_info = iraqi_news_sources[selected_source]

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«
keywords_input = st.sidebar.text_input(
    "ğŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", 
    "",
    help="Ù…Ø«Ø§Ù„: Ø³ÙŠØ§Ø³Ø©ØŒ Ø§Ù‚ØªØµØ§Ø¯ØŒ Ø¨ØºØ¯Ø§Ø¯"
)
keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []

category_filter = st.sidebar.selectbox(
    "ğŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", 
    ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()),
    help="ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø­Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ"
)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®
col_date1, col_date2 = st.sidebar.columns(2)
with col_date1:
    date_from = st.date_input("ğŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today() - timedelta(days=7))
with col_date2:
    date_to = st.date_input("ğŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())

# Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
with st.sidebar.expander("âš™ï¸ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"):
    max_news = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø£Ù‚ØµÙ‰:", 5, 50, 20)
    include_sentiment = st.checkbox("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", True)
    include_categorization = st.checkbox("Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ", True)

run = st.sidebar.button("ğŸ“¥ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", type="primary", help="Ø§Ø¨Ø¯Ø£ Ø¹Ù…Ù„ÙŠØ© Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
if run:
    with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±..."):
        start_time = time.time()
        
        if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
            news = fetch_rss_news(
                selected_source,
                source_info["url"],
                keywords,
                date_from,
                date_to,
                category_filter
            )
        else:
            news = smart_news_fetcher(
                selected_source,
                source_info,
                keywords,
                date_from,
                date_to,
                category_filter
            )
        
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
    
    if news:
        st.success(f"ğŸ‰ ØªÙ… Ø¬Ù„Ø¨ {len(news)} Ø®Ø¨Ø± Ù…Ù† {selected_source} ÙÙŠ {processing_time} Ø«Ø§Ù†ÙŠØ©")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø©
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“° Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", len(news))
        with col2:
            categories = [n['category'] for n in news]
            st.metric("ğŸ“ Ø£ÙƒØ«Ø± ØªØµÙ†ÙŠÙ", Counter(categories).most_common(1)[0][0] if categories else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
        with col3:
            positive_news = len([n for n in news if "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ" in n['sentiment']])
            st.metric("ğŸ˜ƒ Ø£Ø®Ø¨Ø§Ø± Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", positive_news)
        with col4:
            st.metric("â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", f"{processing_time}s")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
        st.subheader("ğŸ“‘ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©")
        
        for i, item in enumerate(news[:max_news], 1):
            with st.container():
                st.markdown(f"### {i}. ğŸ“° {item['title']}")
                
                col_info, col_content = st.columns([1, 2])
                
                with col_info:
                    st.markdown(f"**ğŸ¢ Ø§Ù„Ù…ØµØ¯Ø±:** {item['source']}")
                    st.markdown(f"**ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®:** {item['published'].strftime('%Y-%m-%d %H:%M')}")
                    st.markdown(f"**ğŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ:** {item['category']}")
                    st.markdown(f"**ğŸ­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:** {item['sentiment']}")
                    st.markdown(f"**ğŸ”§ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:** {item.get('extraction_method', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                
                with col_content:
                    st.markdown(f"**ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:** {summarize(item['summary'], 40)}")
                    st.markdown(f"**ğŸ”— [Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„Ø§Ù‹ â†—]({item['link']})**")
                
                if item.get('image'):
                    st.image(item['image'], caption=item['title'], use_column_width=True)
                
                st.markdown("---")
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        st.subheader("ğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        col_export1, col_export2, col_export3 = st.columns(3)
        
        with col_export1:
            word_file = export_to_word(news)
            st.download_button(
                "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Word",
                data=word_file,
                file_name=f"Ø§Ø®Ø¨Ø§Ø±_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        
        with col_export2:
            excel_file = export_to_excel(news)
            st.download_button(
                "ğŸ“Š ØªØ­Ù…ÙŠÙ„ Excel",
                data=excel_file,
                file_name=f"Ø§Ø®Ø¨Ø§Ø±_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        
        with col_export3:
            json_data = json.dumps(news, ensure_ascii=False, default=str, indent=2)
            st.download_button(
                "ğŸ’¾ ØªØ­Ù…ÙŠÙ„ JSON",
                data=json_data.encode('utf-8'),
                file_name=f"Ø§Ø®Ø¨Ø§Ø±_{selected_source}_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                mime="application/json"
            )
        
        # ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
        with st.expander("ğŸ“Š ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"):
            col_analysis1, col_analysis2 = st.columns(2)
            
            with col_analysis1:
                st.subheader("ğŸ“ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª")
                categories = [n['category'] for n in news]
                category_counts = Counter(categories)
                
                for cat, count in category_counts.most_common():
                    percentage = (count / len(news)) * 100
                    st.write(f"â€¢ **{cat}**: {count} ({percentage:.1f}%)")
            
            with col_analysis2:
                st.subheader("ğŸ­ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")
                sentiments = [n['sentiment'] for n in news]
                sentiment_counts = Counter(sentiments)
                
                for sent, count in sentiment_counts.items():
                    percentage = (count / len(news)) * 100
                    st.write(f"â€¢ **{sent}**: {count} ({percentage:.1f}%)")
            
            st.subheader("ğŸ”¤ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹")
            all_text = " ".join([n['title'] + " " + n['summary'] for n in news])
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
            words = re.findall(r'\b[Ø£-ÙŠ]{3,}\b', all_text)  # ÙƒÙ„Ù…Ø§Øª Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
            word_freq = Counter(words).most_common(15)
            
            if word_freq:
                cols = st.columns(3)
                for i, (word, freq) in enumerate(word_freq):
                    with cols[i % 3]:
                        st.write(f"**{word}**: {freq} Ù…Ø±Ø©")
    
    else:
        st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©")
        st.info("ğŸ’¡ Ø¬Ø±Ø¨ ØªÙˆØ³ÙŠØ¹ Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø£Ùˆ ØªØºÙŠÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")
        st.markdown(f"ğŸ”— **[Ø²ÙŠØ§Ø±Ø© {selected_source} Ù…Ø¨Ø§Ø´Ø±Ø©]({source_info['url']})**")

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
st.sidebar.markdown("---")
st.sidebar.info("""
ğŸš€ **ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©:**
- Ø¬Ù„Ø¨ RSS ØªÙ„Ù‚Ø§Ø¦ÙŠ
- ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙˆÙŠØ¨
- ØªØµÙ†ÙŠÙ Ø°ÙƒÙŠ Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
- Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙƒØ±Ø±
""")

st.sidebar.success("âœ… Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù…ØªØ·ÙˆØ± Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±!")

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©
with st.expander("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ‚Ù†ÙŠØ©"):
    st.markdown("""
    ### ğŸ› ï¸ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:
    - **RSS Parsing**: Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
    - **HTML Analysis**: Ù„ØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ÙˆÙŠØ¨ Ù…Ø¨Ø§Ø´Ø±Ø©  
    - **Smart Categorization**: ØªØµÙ†ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø£Ø®Ø¨Ø§Ø±
    - **Sentiment Analysis**: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TextBlob
    - **Regex Extraction**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ù„ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ©
    - **Duplicate Removal**: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ÙƒØ±Ø±Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    
    ### ğŸ“ˆ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
    - **Multi-Method Fetching**: Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø¹Ø¯Ø© Ø·Ø±Ù‚
    - **Fallback System**: Ù†Ø¸Ø§Ù… Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¹Ù†Ø¯ ÙØ´Ù„ RSS
    - **Advanced Filtering**: ÙÙ„ØªØ±Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„ØªØµÙ†ÙŠÙØ§Øª
    - **Real-time Processing**: Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙˆØ±ÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - **Export Options**: ØªØµØ¯ÙŠØ± Ø¨ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø© (Word, Excel, JSON)
    
    ### ğŸ¯ ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù…:
    1. **Ù…Ø­Ø§ÙˆÙ„Ø© RSS Ø£ÙˆÙ„Ø§Ù‹**: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† feeds Ù…ØªØ§Ø­Ø©
    2. **ØªØ­Ù„ÙŠÙ„ HTML**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
    3. **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ©**: ØªÙ†Ø¸ÙŠÙ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    4. **Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±**: Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
    5. **ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±
    """)
