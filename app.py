import streamlit as st
import feedparser
import pandas as pd
from datetime import datetime
from io import BytesIO
from textblob import TextBlob
from collections import Counter
from docx import Document
import requests
from bs4 import BeautifulSoup

st.set_page_config(page_title="ğŸ“° Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø°ÙƒÙŠØ©", layout="wide")
st.title("ğŸ—ï¸ Ø£Ø¯Ø§Ø© Ø¥Ø¯Ø§Ø±Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© + Ù…ØµØ§Ø¯Ø± Ø£ÙƒØ«Ø±)")

# Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
category_keywords = {
    "Ø³ÙŠØ§Ø³Ø©": ["Ø±Ø¦ÙŠØ³", "ÙˆØ²ÙŠØ±", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø¨Ø±Ù„Ù…Ø§Ù†", "Ø³ÙŠØ§Ø³Ø©", "Ø­ÙƒÙˆÙ…Ø©", "Ù†Ø§Ø¦Ø¨"],
    "Ø±ÙŠØ§Ø¶Ø©": ["ÙƒØ±Ø©", "Ù„Ø§Ø¹Ø¨", "Ù…Ø¨Ø§Ø±Ø§Ø©", "Ø¯ÙˆØ±ÙŠ", "Ù‡Ø¯Ù", "ÙØ±ÙŠÙ‚", "Ø¨Ø·ÙˆÙ„Ø©"],
    "Ø§Ù‚ØªØµØ§Ø¯": ["Ø³ÙˆÙ‚", "Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ø³ØªØ«Ù…Ø§Ø±", "Ø¨Ù†Ùƒ", "Ù…Ø§Ù„", "ØªØ¬Ø§Ø±Ø©", "ØµÙ†Ø§Ø¹Ø©"],
    "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": ["ØªÙ‚Ù†ÙŠØ©", "ØªØ·Ø¨ÙŠÙ‚", "Ù‡Ø§ØªÙ", "Ø°ÙƒØ§Ø¡", "Ø¨Ø±Ù…Ø¬Ø©", "Ø¥Ù†ØªØ±Ù†Øª", "Ø±Ù‚Ù…ÙŠ"]
}

# Ø§Ù„Ø¯ÙˆØ§Ù„
def summarize(text, max_words=25):
    return " ".join(text.split()[:max_words]) + "..."

def analyze_sentiment(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0.1:
        return "ğŸ˜ƒ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif polarity < -0.1:
        return "ğŸ˜  Ø³Ù„Ø¨ÙŠ"
    else:
        return "ğŸ˜ Ù…Ø­Ø§ÙŠØ¯"

def detect_category(text):
    for category, words in category_keywords.items():
        if any(word in text for word in words):
            return category
    return "ØºÙŠØ± Ù…ØµÙ†Ù‘Ù"

def fetch_rss_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…ØµØ§Ø¯Ø± RSS"""
    try:
        feed = feedparser.parse(url)
        news_list = []
        for entry in feed.entries:
            title = entry.title
            summary = entry.get("summary", "")
            link = entry.link
            published = entry.get("published", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            try:
                published_dt = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S %Z")
            except:
                try:
                    published_dt = datetime.strptime(published, "%Y-%m-%dT%H:%M:%S%z")
                except:
                    published_dt = datetime.now()
            
            image = ""
            if 'media_content' in entry:
                image = entry.media_content[0].get('url', '')
            elif 'media_thumbnail' in entry:
                image = entry.media_thumbnail[0].get('url', '')

            if not (date_from <= published_dt.date() <= date_to):
                continue

            full_text = title + " " + summary
            if keywords and not any(k.lower() in full_text.lower() for k in keywords):
                continue

            auto_category = detect_category(full_text)
            if chosen_category != "Ø§Ù„ÙƒÙ„" and auto_category != chosen_category:
                continue

            news_list.append({
                "source": source_name,
                "title": title,
                "summary": summary,
                "link": link,
                "published": published_dt,
                "image": image,
                "sentiment": analyze_sentiment(summary),
                "category": auto_category
            })

        return news_list
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}: {str(e)}")
        return []

def fetch_website_news(source_name, url, keywords, date_from, date_to, chosen_category):
    """Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© (Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ¯Ø¹Ù… RSS)"""
    try:
        # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ©
        # ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹ Ø¹Ù„Ù‰ Ø­Ø¯Ø©
        return []
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† {source_name}: {str(e)}")
        return []

def export_to_word(news_list):
    doc = Document()
    doc.add_heading('ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 0)
    
    for news in news_list:
        doc.add_heading(news['title'], level=2)
        doc.add_paragraph(f"Ø§Ù„Ù…ØµØ¯Ø±: {news['source']}  |  Ø§Ù„ØªØµÙ†ÙŠÙ: {news['category']}")
        doc.add_paragraph(f"ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {news['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        doc.add_paragraph(f"ğŸ“„ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {summarize(news['summary'])}")
        doc.add_paragraph(f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: {news['link']}")
        doc.add_paragraph(f"ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù†ÙˆÙŠ: {news['sentiment']}")
        doc.add_paragraph("-----")
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def export_to_excel(news_list):
    df = pd.DataFrame(news_list)
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø©
general_rss_feeds = {
    "BBC Ø¹Ø±Ø¨ÙŠ": "http://feeds.bbci.co.uk/arabic/rss.xml",
    "CNN Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "http://arabic.cnn.com/rss/latest",
    "RT Arabic": "https://arabic.rt.com/rss/",
    "France24 Ø¹Ø±Ø¨ÙŠ": "https://www.france24.com/ar/rss",
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø·": "https://aawsat.com/home/rss.xml",
    "Ø³ÙƒØ§ÙŠ Ù†ÙŠÙˆØ² Ø¹Ø±Ø¨ÙŠØ©": "https://www.skynewsarabia.com/web/rss",
    "Ø§Ù„Ø¬Ø²ÙŠØ±Ø©": "https://www.aljazeera.net/aljazeerarss/ar/home",
    "Ø¹Ø±Ø¨ÙŠ21": "https://arabi21.com/feed",
    "Ø§Ù„ÙˆØ·Ù†": "https://www.elwatannews.com/home/rss",
    "Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø³Ø§Ø¨Ø¹": "https://www.youm7.com/rss/SectionRss?SectionID=65",
    "Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„ÙŠÙˆÙ…": "https://www.almasryalyoum.com/rss/rssfeeds",
    "ØµØ­ÙŠÙØ© Ø³Ø¨Ù‚": "https://sabq.org/rss"
}

# âœ… Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
iraqi_news_sources = {
    "Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://hathalyoum.net/",
        "type": "website",
        "rss": "https://hathalyoum.net/feed/"  # Ù…Ø­Ø§ÙˆÙ„Ø© RSS
    },
    "Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ…": {
        "url": "https://iraqtoday.com/",
        "type": "website", 
        "rss": "https://iraqtoday.com/feed/"  # Ù…Ø­Ø§ÙˆÙ„Ø© RSS
    },
    "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://moi.gov.iq/",
        "type": "website",
        "rss": None
    },
    "Ø±Ø¦Ø§Ø³Ø© Ø§Ù„Ø¬Ù…Ù‡ÙˆØ±ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©": {
        "url": "https://presidency.iq/",
        "type": "website",
        "rss": "https://presidency.iq/feed/"  # Ù…Ø­Ø§ÙˆÙ„Ø© RSS
    },
    "Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://asharq.com/tags/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss": None
    },
    "RT Arabic - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://arabic.rt.com/focuses/10744-%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss": "https://arabic.rt.com/rss/focuses/10744"  # Ù…Ø­Ø§ÙˆÙ„Ø© RSS
    },
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://x.com/AlArabiya_Iraq",
        "type": "social",
        "rss": None
    },
    "Ø´Ø¨ÙƒØ© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ": {
        "url": "https://x.com/iraqmedianet",
        "type": "social",
        "rss": None
    },
    "Ø¥Ù†Ø¯Ø¨Ù†Ø¯Ù†Øª Ø¹Ø±Ø¨ÙŠØ© - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://www.independentarabia.com/tags/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82",
        "type": "website",
        "rss": None
    },
    "ÙØ±Ø§Ù†Ø³ 24 - Ø§Ù„Ø¹Ø±Ø§Ù‚": {
        "url": "https://www.france24.com/ar/%D8%AA%D8%A7%D8%BA/%D8%A7%D9%84%D8%B9%D8%B1%D8%A7%D9%82/",
        "type": "website",
        "rss": "https://www.france24.com/ar/rss"  # RSS Ø¹Ø§Ù…
    }
}

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­ÙƒÙ…
st.sidebar.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«")

# Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±
source_type = st.sidebar.selectbox(
    "ğŸŒ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„Ù…ØµØ¯Ø±:",
    ["Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©", "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©"]
)

if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
    selected_source = st.sidebar.selectbox("ğŸŒ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±:", list(general_rss_feeds.keys()))
    source_url = general_rss_feeds[selected_source]
    source_info = {"type": "rss", "url": source_url}
else:
    selected_source = st.sidebar.selectbox("ğŸ‡®ğŸ‡¶ Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠ:", list(iraqi_news_sources.keys()))
    source_info = iraqi_news_sources[selected_source]

keywords_input = st.sidebar.text_input("ğŸ” ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© (Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„):", "")
keywords = [kw.strip() for kw in keywords_input.split(",")] if keywords_input else []

category_filter = st.sidebar.selectbox("ğŸ“ Ø§Ø®ØªØ± Ø§Ù„ØªØµÙ†ÙŠÙ:", ["Ø§Ù„ÙƒÙ„"] + list(category_keywords.keys()))

date_from = st.sidebar.date_input("ğŸ“… Ù…Ù† ØªØ§Ø±ÙŠØ®:", datetime.today())
date_to = st.sidebar.date_input("ğŸ“… Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®:", datetime.today())

run = st.sidebar.button("ğŸ“¥ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±", type="primary")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
col1, col2 = st.columns([2, 1])

with col1:
    if run:
        with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±..."):
            news = []
            
            if source_type == "Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù…Ø©":
                news = fetch_rss_news(
                    selected_source,
                    source_info["url"],
                    keywords,
                    date_from,
                    date_to,
                    category_filter
                )
            else:  # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ©
                if source_info.get("rss"):
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ RSS Ø£ÙˆÙ„Ø§Ù‹
                    news = fetch_rss_news(
                        selected_source,
                        source_info["rss"],
                        keywords,
                        date_from,
                        date_to,
                        category_filter
                    )
                
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙ†Ø¬Ø­ RSS Ø£Ùˆ Ù„Ù… ÙŠÙƒÙ† Ù…ØªÙˆÙØ±Ø§Ù‹
                if not news:
                    if source_info["type"] == "social":
                        st.warning("âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù…Ù†ØµØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø©.")
                        st.markdown(f"ğŸ”— [Ø²ÙŠØ§Ø±Ø© {selected_source}]({source_info['url']})")
                    else:
                        news = fetch_website_news(
                            selected_source,
                            source_info["url"],
                            keywords,
                            date_from,
                            date_to,
                            category_filter
                        )
                        if not news:
                            st.info("â„¹ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ØµØ¯Ø±. Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ·ÙˆÙŠØ± Ø®Ø§Øµ Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…Ø­ØªÙˆÙ‰.")
                            st.markdown(f"ğŸ”— [Ø²ÙŠØ§Ø±Ø© {selected_source}]({source_info['url']})")

        if news:
            st.success(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(news)} Ø®Ø¨Ø± Ù…Ù† {selected_source}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±
            for i, item in enumerate(news):
                with st.expander(f"ğŸ“° {item['title'][:100]}..."):
                    cols = st.columns([1, 3])
                    with cols[0]:
                        if item.get("image"):
                            try:
                                st.image(item["image"], use_column_width=True)
                            except:
                                st.write("ğŸ–¼ï¸ ØµÙˆØ±Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø©")
                    
                    with cols[1]:
                        st.markdown(f"**ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®:** {item['published'].strftime('%Y-%m-%d %H:%M')}")
                        st.markdown(f"**ğŸ“ Ø§Ù„ØªØµÙ†ÙŠÙ:** {item['category']}")
                        st.markdown(f"**ğŸ¢ Ø§Ù„Ù…ØµØ¯Ø±:** {item['source']}")
                        st.markdown(f"**ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:** {summarize(item['summary'], 50)}")
                        st.markdown(f"**ğŸ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ:** {item['sentiment']}")
                        st.markdown(f"**ğŸ”— [Ø§Ù‚Ø±Ø£ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„Ø§Ù‹]({item['link']})**")

        elif run:
            st.warning("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø¨Ø§Ø± Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙˆØ· Ø£Ùˆ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
with col2:
    if run and news:
        st.subheader("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        categories = [n['category'] for n in news]
        category_counts = Counter(categories)
        
        st.write("**ğŸ“ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:**")
        for cat, count in category_counts.items():
            st.write(f"- {cat}: {count}")
        
        # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiments = [n['sentiment'] for n in news]
        sentiment_counts = Counter(sentiments)
        
        st.write("**ğŸ­ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:**")
        for sent, count in sentiment_counts.items():
            st.write(f"- {sent}: {count}")
        
        # Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹
        st.write("**ğŸ”¤ Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹:**")
        all_text = " ".join([n['summary'] for n in news])
        words = [word for word in all_text.split() if len(word) > 3]
        word_freq = Counter(words).most_common(10)
        for word, freq in word_freq:
            st.write(f"- **{word}**: {freq}")

# Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
if run and news:
    st.subheader("ğŸ“¥ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    col_export1, col_export2 = st.columns(2)
    
    with col_export1:
        word_file = export_to_word(news)
        st.download_button(
            "ğŸ“„ ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Word",
            data=word_file,
            file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d')}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
    
    with col_export2:
        excel_file = export_to_excel(news)
        st.download_button(
            "ğŸ“Š ØªØ­Ù…ÙŠÙ„ ÙƒÙ€ Excel",
            data=excel_file,
            file_name=f"news_{selected_source}_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

# Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
st.sidebar.markdown("---")
st.sidebar.info("""
ğŸ’¡ **Ù…Ù„Ø§Ø­Ø¸Ø§Øª:**
- Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ÙˆÙ‚Øª Ø£Ø·ÙˆÙ„ Ù„Ù„ØªØ­Ù…ÙŠÙ„
- Ù…Ù†ØµØ§Øª Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ØªØªØ·Ù„Ø¨ Ø²ÙŠØ§Ø±Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
- ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©
""")

st.sidebar.success("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¨Ù…ØµØ§Ø¯Ø± Ø¹Ø±Ø§Ù‚ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©!")
